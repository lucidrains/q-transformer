{
    "100": {
        "file_id": 7,
        "content": "/q_transformer/q_robotic_transformer.py",
        "type": "filepath"
    },
    "101": {
        "file_id": 7,
        "content": "This code contains utility functions for tensor operations, Residual Blocks, and QHeadMultipleActions module. It includes MaxViT and Robotic Transformer classes, implementing a Q-Transformer model with configurable parameters for single or multi-action scenarios.",
        "type": "summary"
    },
    "102": {
        "file_id": 7,
        "content": "from random import random\nfrom functools import partial, cache\nimport torch\nimport torch.nn.functional as F\nimport torch.distributed as dist\nfrom torch.cuda.amp import autocast\nfrom torch import nn, einsum, Tensor\nfrom torch.nn import Module, ModuleList\nfrom beartype import beartype\nfrom beartype.typing import Union, List, Optional, Callable, Tuple, Dict, Any\nfrom einops import pack, unpack, repeat, reduce, rearrange\nfrom einops.layers.torch import Rearrange, Reduce\nfrom q_transformer.attend import Attend\nfrom classifier_free_guidance_pytorch import (\n    TextConditioner,\n    AttentionTextConditioner,\n    NullConditioner,\n    classifier_free_guidance\n)\n# helpers\ndef exists(val):\n    return val is not None\ndef xnor(x, y):\n    \"\"\" (True, True) or (False, False) -> True \"\"\"\n    return not (x ^ y)\ndef divisible_by(num, den):\n    return (num % den) == 0\ndef default(val, d):\n    return val if exists(val) else d\ndef cast_tuple(val, length = 1):\n    return val if isinstance(val, tuple) else ((val,) * length)\n# tensor helpers",
        "type": "code",
        "location": "/q_transformer/q_robotic_transformer.py:1-44"
    },
    "103": {
        "file_id": 7,
        "content": "This code imports necessary libraries and modules, and defines several utility functions for working with tensors and other data types. It also includes helper functions for type checking and tensor operations. The code appears to be part of a larger machine learning or artificial intelligence program involving transformers and robotic systems.",
        "type": "comment"
    },
    "104": {
        "file_id": 7,
        "content": "def l2norm(t, dim = -1):\n    return F.normalize(t, dim = dim)\ndef pack_one(x, pattern):\n    return pack([x], pattern)\ndef unpack_one(x, ps, pattern):\n    return unpack(x, ps, pattern)[0]\n# 2d rotary positional embedding\n# https://arxiv.org/abs/2104.09864\nclass RotaryEmbedding(Module):\n    def __init__(self, dim, omega = 10000):\n        super().__init__()\n        inv_freq = 1.0 / (omega ** (torch.arange(0, dim, 4).float() / dim))\n        self.register_buffer('inv_freq', inv_freq)\n    @autocast(enabled = False)\n    def forward(self, height_width):\n        device, dtype = self.inv_freq.device, self.inv_freq.dtype\n        axial_pos = torch.arange(height_width, device = device).type(dtype)\n        freqs = torch.einsum('i, j -> i j', axial_pos, self.inv_freq)\n        freqs = repeat(freqs, '... f -> ... (f c)', c = 2)\n        freqs = torch.broadcast_tensors(freqs[None, :, :], freqs[:, None, :])\n        freqs = torch.cat(freqs, dim = -1)\n        return rearrange(freqs, '... f -> (...) f')\ndef rotate_half(x):\n    x1, x2 = rearrange(x, '... (d c) -> ... d c', c = 2).unbind(dim = -1)",
        "type": "code",
        "location": "/q_transformer/q_robotic_transformer.py:46-78"
    },
    "105": {
        "file_id": 7,
        "content": "The code defines a class `RotaryEmbedding` for 2D rotary positional embeddings and contains various utility functions. The `l2norm()` function calculates the L2-normalized version of a tensor, while `pack_one()` and `unpack_one()` are used to pack and unpack tensors with a specific pattern. The `RotaryEmbedding` class initializes an instance with dimensionality (`dim`) and frequency scaling factor (`omega`), then computes the rotary embeddings for the given height and width in the forward pass using the Arctan function. The `rotate_half()` function rearranges a tensor to have half dimensions followed by the other half.",
        "type": "comment"
    },
    "106": {
        "file_id": 7,
        "content": "    x = torch.stack((-x2, x1), dim = -1)\n    return rearrange(x, '... d c -> ... (d c)')\n@autocast(enabled = False)\ndef apply_rotary_pos_emb(pos, t):\n    return t * pos.cos() + rotate_half(t) * pos.sin()\n# sync batchnorm\n@cache\ndef get_is_distributed():\n    return dist.is_initialized() and dist.get_world_size() > 1\ndef MaybeSyncBatchnorm2d(is_distributed = None):\n    is_distributed = default(is_distributed, get_is_distributed())\n    return nn.SyncBatchNorm if is_distributed else nn.BatchNorm2d\n# channel rmsnorm\nclass RMSNorm(Module):\n    def __init__(self, dim, affine = True):\n        super().__init__()\n        self.scale = dim ** 0.5\n        self.gamma = nn.Parameter(torch.ones(dim)) if affine else 1.\n    def forward(self, x):\n        return l2norm(x) * self.gamma * self.scale\nclass ChanRMSNorm(Module):\n    def __init__(self, dim, affine = True):\n        super().__init__()\n        self.scale = dim ** 0.5\n        self.gamma = nn.Parameter(torch.ones(dim, 1, 1)) if affine else 1.\n    def forward(self, x):\n        return l2norm(x, dim = 1) * self.gamma * self.scale",
        "type": "code",
        "location": "/q_transformer/q_robotic_transformer.py:79-114"
    },
    "107": {
        "file_id": 7,
        "content": "This code snippet contains various functions and classes. It includes a function to apply rotary position embeddings, sync batch normalization with distributed training, and two custom normalization layers: RMSNorm and ChanRMSNorm. The RMSNorm class performs channel-wise L2 normalization while the ChanRMSNorm class performs spatial and channel-wise L2 normalization.",
        "type": "comment"
    },
    "108": {
        "file_id": 7,
        "content": "# sinusoidal positions\ndef posemb_sincos_1d(seq, dim, temperature = 10000, device = None, dtype = torch.float32):\n    n = torch.arange(seq, device = device)\n    omega = torch.arange(dim // 2, device = device) / (dim // 2 - 1)\n    omega = 1. / (temperature ** omega)\n    n = n[:, None] * omega[None, :]\n    pos_emb = torch.cat((n.sin(), n.cos()), dim = 1)\n    return pos_emb.type(dtype)\n# helper classes\nclass Residual(Module):\n    @beartype\n    def __init__(self, fn: Module):\n        super().__init__()\n        self.fn = fn\n    def forward(self, x, **kwargs):\n        return self.fn(x, **kwargs) + x\nclass FeedForward(Module):\n    def __init__(\n        self,\n        dim,\n        mult = 4,\n        dropout = 0.,\n        adaptive_ln = False\n    ):\n        super().__init__()\n        self.adaptive_ln = adaptive_ln\n        inner_dim = int(dim * mult)\n        self.norm = RMSNorm(dim, affine = not adaptive_ln)\n        self.net = nn.Sequential(\n            nn.Linear(dim, inner_dim),\n            nn.GELU(),\n            nn.Dropout(dropout),",
        "type": "code",
        "location": "/q_transformer/q_robotic_transformer.py:116-155"
    },
    "109": {
        "file_id": 7,
        "content": "This code defines a function called posemb_sincos_1d that computes sine and cosine values for positional embeddings in one dimension. It also includes two helper classes, Residual and FeedForward, which are used for building the residual blocks and feed-forward layers of a transformer model. The Residual class is a wrapper around another module that allows for skip connections, while the FeedForward class represents the standard feed-forward layer with normalization and dropout options.",
        "type": "comment"
    },
    "110": {
        "file_id": 7,
        "content": "            nn.Linear(inner_dim, dim),\n            nn.Dropout(dropout)\n        )\n    def forward(\n        self,\n        x,\n        cond_fn: Optional[Callable] = None\n    ):\n        x = self.norm(x)\n        assert xnor(self.adaptive_ln, exists(cond_fn))\n        if exists(cond_fn):\n            # adaptive layernorm\n            x = cond_fn(x)\n        return self.net(x)\n# MBConv\nclass SqueezeExcitation(Module):\n    def __init__(self, dim, shrinkage_rate = 0.25):\n        super().__init__()\n        hidden_dim = int(dim * shrinkage_rate)\n        self.gate = nn.Sequential(\n            Reduce('b c h w -> b c', 'mean'),\n            nn.Linear(dim, hidden_dim, bias = False),\n            nn.SiLU(),\n            nn.Linear(hidden_dim, dim, bias = False),\n            nn.Sigmoid(),\n            Rearrange('b c -> b c 1 1')\n        )\n    def forward(self, x):\n        return x * self.gate(x)\nclass MBConvResidual(Module):\n    def __init__(self, fn, dropout = 0.):\n        super().__init__()\n        self.fn = fn\n        self.dropsample = Dropsample(dropout)",
        "type": "code",
        "location": "/q_transformer/q_robotic_transformer.py:156-198"
    },
    "111": {
        "file_id": 7,
        "content": "The code defines a Residual Block for a Convolutional Neural Network (CNN) using the Mobile Inverted Bottleneck Convolution (MBConv) architecture. It consists of a Sequential module with layers such as Linear, Dropout, Reduce, SiLU (Simplified Linear Unit), Sigmoid, and Rearrange. The MBConvResidual class initializes the Residual block by taking the MBConv architecture and optionally applying dropout.",
        "type": "comment"
    },
    "112": {
        "file_id": 7,
        "content": "    def forward(self, x):\n        out = self.fn(x)\n        out = self.dropsample(out)\n        return out + x\nclass Dropsample(Module):\n    def __init__(self, prob = 0):\n        super().__init__()\n        self.prob = prob\n    def forward(self, x):\n        batch, device = x.shape[0], x.device\n        if self.prob == 0. or (not self.training):\n            return x\n        keep_mask = torch.FloatTensor((batch, 1, 1, 1), device = device).uniform_() > self.prob\n        return x * keep_mask / (1 - self.prob)\ndef MBConv(\n    dim_in,\n    dim_out,\n    *,\n    downsample,\n    expansion_rate = 4,\n    shrinkage_rate = 0.25,\n    dropout = 0.,\n    is_distributed = None,\n    use_layernorm = True\n):\n    hidden_dim = int(expansion_rate * dim_out)\n    stride = 2 if downsample else 1\n    if use_layernorm:\n        norm_klass = ChanRMSNorm\n    else:\n        norm_klass = MaybeSyncBatchnorm2d(is_distributed)\n    net = nn.Sequential(\n        nn.Conv2d(dim_in, hidden_dim, 1),\n        norm_klass(hidden_dim),\n        nn.GELU(),\n        nn.Conv2d(hidden_dim, hidden_dim, 3, stride = stride, padding = 1, groups = hidden_dim),",
        "type": "code",
        "location": "/q_transformer/q_robotic_transformer.py:200-242"
    },
    "113": {
        "file_id": 7,
        "content": "The code defines a function \"forward\" within the class \"q_robotic_transformer\", which applies a transformation to the input. It also includes the definition for a module named \"Dropsample\", that has the ability to drop some samples with a specified probability. The \"MBConv\" function is defined to create a type of convolutional neural network block, taking in dimensions, downsample factor, and other parameters.",
        "type": "comment"
    },
    "114": {
        "file_id": 7,
        "content": "        norm_klass(hidden_dim),\n        nn.GELU(),\n        SqueezeExcitation(hidden_dim, shrinkage_rate = shrinkage_rate),\n        nn.Conv2d(hidden_dim, dim_out, 1),\n        norm_klass(dim_out)\n    )\n    if dim_in == dim_out and not downsample:\n        net = MBConvResidual(net, dropout = dropout)\n    return net\n# attention related classes\nclass Attention(Module):\n    def __init__(\n        self,\n        dim,\n        heads = 8,\n        dim_head = 32,\n        dropout = 0.,\n        window_size = 7,\n        num_mem_kv = 4,\n        flash = True\n    ):\n        super().__init__()\n        dim_inner = dim_head * heads\n        self.norm = RMSNorm(dim)\n        self.heads = heads\n        self.to_qkv = nn.Linear(dim, dim_inner * 3, bias = False)\n        self.to_v_gates = nn.Sequential(\n            nn.Linear(dim, self.heads),\n            nn.Sigmoid(),\n            Rearrange('b n h -> b h n 1')\n        )\n        self.attend = Attend(\n            causal = False,\n            dropout = dropout,\n            flash = flash\n        )\n        self.to_out = nn.Sequential(",
        "type": "code",
        "location": "/q_transformer/q_robotic_transformer.py:243-288"
    },
    "115": {
        "file_id": 7,
        "content": "The code defines a function that takes in an input size (dim_in) and an output size (dim_out), as well as other parameters such as dropout rate, and creates a series of layers including convolutions, normalization, and activation functions. If the input and output sizes are the same and there is no downsampling, it applies a residual connection to the resulting network before returning it. The code also defines an Attention class that performs multi-head self-attention with various parameters such as window size and number of memory key/value pairs.",
        "type": "comment"
    },
    "116": {
        "file_id": 7,
        "content": "            nn.Linear(dim_inner, dim, bias = False),\n            nn.Dropout(dropout)\n        )\n    def forward(\n        self,\n        x,\n        rotary_emb = None\n    ):\n        batch, height, width, window_height, window_width, _, device, h = *x.shape, x.device, self.heads\n        x = self.norm(x)\n        # flatten\n        x = rearrange(x, 'b x y w1 w2 d -> (b x y) (w1 w2) d')\n        # project for queries, keys, values\n        q, k, v = self.to_qkv(x).chunk(3, dim = -1)\n        g = self.to_v_gates(x)\n        # split heads\n        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), (q, k, v))\n        # 2d rotary\n        if exists(rotary_emb):\n            q = apply_rotary_pos_emb(rotary_emb, q)\n            k = apply_rotary_pos_emb(rotary_emb, k)\n        # attention\n        out = self.attend(q, k, v)\n        # gate values per head, allow for attending to nothing\n        out = out * g\n        # merge heads\n        out = rearrange(out, 'b h (w1 w2) d -> b w1 w2 (h d)', w1 = window_height, w2 = window_width)",
        "type": "code",
        "location": "/q_transformer/q_robotic_transformer.py:289-332"
    },
    "117": {
        "file_id": 7,
        "content": "The code defines a QTransformer module that performs multi-head self-attention and includes a gate mechanism. It takes input tensor x, optional rotary embedding (rotary_emb), and applies normalization to the input. The input is then flattened, split into queries, keys, and values for each head, and passed through separate projection layers. If rotary_emb is provided, it applies 2D rotary position embeddings to queries and keys. It calculates attention scores using attend function and multiplies the output by gating values. Finally, it merges heads back into a single tensor with specified window dimensions.",
        "type": "comment"
    },
    "118": {
        "file_id": 7,
        "content": "        # combine heads out\n        out = self.to_out(out)\n        return rearrange(out, '(b x y) ... -> b x y ...', x = height, y = width)\nclass MaxViT(Module):\n    @beartype\n    def __init__(\n        self,\n        *,\n        num_classes,\n        dim,\n        depth: Tuple[int, ...],\n        heads = 8,\n        dim_head = 64,\n        dim_conv_stem = None,\n        window_size = 7,\n        mbconv_expansion_rate = 4,\n        mbconv_shrinkage_rate = 0.25,\n        use_layernorm = True,\n        dropout = 0.1,\n        channels = 3,\n        flash_attn = True\n    ):\n        super().__init__()\n        # convolutional stem\n        dim_conv_stem = default(dim_conv_stem, dim)\n        self.conv_stem = nn.Sequential(\n            nn.Conv2d(channels, dim_conv_stem, 3, stride = 2, padding = 1),\n            nn.Conv2d(dim_conv_stem, dim_conv_stem, 3, padding = 1)\n        )\n        # variables\n        num_stages = len(depth)\n        dims = tuple(map(lambda i: (2 ** i) * dim, range(num_stages)))\n        dims = (dim_conv_stem, *dims)\n        dim_pairs = tuple(zip(dims[:-1], dims[1:]))",
        "type": "code",
        "location": "/q_transformer/q_robotic_transformer.py:334-374"
    },
    "119": {
        "file_id": 7,
        "content": "This code defines a MaxViT class that takes various parameters and initializes a convolutional stem for image processing. The dim_pairs are zipped from the provided dimensions list, likely for subsequent use in the network architecture.",
        "type": "comment"
    },
    "120": {
        "file_id": 7,
        "content": "        self.layers = ModuleList([])\n        # shorthand for window size for efficient block - grid like attention\n        self.window_size = window_size\n        w = window_size\n        # rotary embedding\n        assert divisible_by(dim_head, 4), f'{dim_head} must be divisible by 4 for axial rotary embedding for maxvit'\n        self.axial_rotary_emb = RotaryEmbedding(dim_head)\n        self.register_buffer('cached_rotary_emb', self.axial_rotary_emb(window_size), persistent = False)\n        # iterate through stages\n        cond_hidden_dims = []\n        for ind, ((layer_dim_in, layer_dim), layer_depth) in enumerate(zip(dim_pairs, depth)):\n            for stage_ind in range(layer_depth):\n                is_first = stage_ind == 0\n                stage_dim_in = layer_dim_in if is_first else layer_dim\n                cond_hidden_dims.append(stage_dim_in)\n                block = nn.ModuleList([\n                    MBConv(\n                        stage_dim_in,\n                        layer_dim,\n                        downsample = is_first,",
        "type": "code",
        "location": "/q_transformer/q_robotic_transformer.py:376-405"
    },
    "121": {
        "file_id": 7,
        "content": "This code initializes a MaxViT module by creating ModuleList, setting window size and rotary embedding, and iterating through stages to create MBConv layers.",
        "type": "comment"
    },
    "122": {
        "file_id": 7,
        "content": "                        expansion_rate = mbconv_expansion_rate,\n                        shrinkage_rate = mbconv_shrinkage_rate,\n                        use_layernorm = use_layernorm\n                    ),\n                    Rearrange('b d (x w1) (y w2) -> b x y w1 w2 d', w1 = w, w2 = w),  # block-like attention\n                    Residual(Attention(dim = layer_dim, heads = heads, dim_head = dim_head, dropout = dropout, window_size = w, flash = flash_attn)),\n                    Residual(FeedForward(dim = layer_dim, dropout = dropout)),\n                    Rearrange('b x y w1 w2 d -> b d (x w1) (y w2)'),\n                    Rearrange('b d (w1 x) (w2 y) -> b x y w1 w2 d', w1 = w, w2 = w),  # grid-like attention\n                    Residual(Attention(dim = layer_dim, heads = heads, dim_head = dim_head, dropout = dropout, window_size = w, flash = flash_attn)),\n                    Residual(FeedForward(dim = layer_dim, dropout = dropout)),\n                    Rearrange('b x y w1 w2 d -> b d (w1 x) (w2 y)'),",
        "type": "code",
        "location": "/q_transformer/q_robotic_transformer.py:406-418"
    },
    "123": {
        "file_id": 7,
        "content": "This code defines a neural network layer with multiple operations. It includes a separable convolution, block-like attention, grid-like attention, and feedforward layers. The layer has parameters for expansion rate, shrinkage rate, using layernorm, window size, heads, dimension, dropout, and flash attention. The Rearrange operations reshape the input tensor at specific points in the layer.",
        "type": "comment"
    },
    "124": {
        "file_id": 7,
        "content": "                ])\n                self.layers.append(block)\n        embed_dim = dims[-1]\n        self.embed_dim = dims[-1]\n        self.cond_hidden_dims = cond_hidden_dims\n        # mlp head out\n        self.mlp_head = nn.Sequential(\n            Reduce('b d h w -> b d', 'mean'),\n            RMSNorm(embed_dim),\n            nn.Linear(embed_dim, num_classes)\n        )\n    @beartype\n    def forward(\n        self,\n        img,\n        texts: Optional[List[str]] = None,\n        cond_fns: Optional[Tuple[Callable, ...]] = None,\n        cond_drop_prob = 0.,\n        return_embeddings = False\n    ):\n        assert all([divisible_by(d, self.window_size) for d in img.shape[-2:]])\n        x = self.conv_stem(img)\n        rotary_emb = self.cached_rotary_emb\n        cond_fns = iter(default(cond_fns, []))\n        for (\n            mb_conv,\n            rearr_windowed_in,\n            windowed_attn,\n            windowed_ff,\n            rearr_windowed_out,\n            rearr_grid_in,\n            grid_attn,\n            grid_ff,\n            rearr_grid_out",
        "type": "code",
        "location": "/q_transformer/q_robotic_transformer.py:419-462"
    },
    "125": {
        "file_id": 7,
        "content": "This code defines a Q-Transformer model for robotics, with convolutional stem, window-based self-attention, and multi-layer perceptron head. It takes in an image and optionally text input (token list) with conditional functions. The model processes the inputs through multiple layers, applying convolutions, windowed attention, feedforward networks, and rearrangements. It also includes a Reduce operation to calculate mean across dimensions, followed by RMSNorm for normalization. Finally, it uses Linear layer to map from embedding dimension to number of classes.",
        "type": "comment"
    },
    "126": {
        "file_id": 7,
        "content": "        ) in self.layers:\n            cond_fn = next(cond_fns, None)\n            if exists(cond_fn):\n                x = cond_fn(x)\n            x = mb_conv(x)\n            x = rearr_windowed_in(x)\n            x = windowed_attn(x, rotary_emb = rotary_emb)\n            x = windowed_ff(x)\n            x = rearr_windowed_out(x)\n            x = rearr_grid_in(x)\n            x = grid_attn(x, rotary_emb = rotary_emb)\n            x = grid_ff(x)\n            x = rearr_grid_out(x)\n        if return_embeddings:\n            return x\n        return self.mlp_head(x)\n# attention\nclass TransformerAttention(Module):\n    def __init__(\n        self,\n        dim,\n        dim_head = 64,\n        dim_context = None,\n        heads = 8,\n        num_mem_kv = 4,\n        norm_context = False,\n        adaptive_ln = False,\n        dropout = 0.1,\n        flash = True,\n        causal = False\n    ):\n        super().__init__()\n        self.heads = heads\n        inner_dim = dim_head * heads\n        dim_context = default(dim_context, dim)\n        self.adaptive_ln = adaptive_ln",
        "type": "code",
        "location": "/q_transformer/q_robotic_transformer.py:463-507"
    },
    "127": {
        "file_id": 7,
        "content": "The code defines a Robotic Transformer for window and grid-based attention. It iterates through layers, performs convolution, windowed attention, windowed feedforward, rearrangements, grid attention, and grid feedforward. The function returns the last layer output if return_embeddings is True, otherwise it passes the output through an MLP head and returns it. Additionally, the code defines a TransformerAttention class that initializes a transformer attention module with specified dimensions and parameters.",
        "type": "comment"
    },
    "128": {
        "file_id": 7,
        "content": "        self.norm = RMSNorm(dim, affine = not adaptive_ln)\n        self.context_norm = RMSNorm(dim_context) if norm_context else None\n        self.attn_dropout = nn.Dropout(dropout)\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim_context, inner_dim * 2, bias = False)\n        self.num_mem_kv = num_mem_kv\n        self.mem_kv = None\n        if num_mem_kv > 0:\n            self.mem_kv = nn.Parameter(torch.randn(2, heads, num_mem_kv, dim_head))\n        self.attend = Attend(\n            dropout = dropout,\n            flash = flash,\n            causal = causal\n        )\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            nn.Dropout(dropout)\n        )\n    def forward(\n        self,\n        x,\n        context = None,\n        mask = None,\n        attn_mask = None,\n        cond_fn: Optional[Callable] = None,\n        cache: Optional[Tensor] = None,\n        return_cache = False\n    ):\n        b = x.shape[0]\n        assert xnor(exists(context), exists(self.context_norm))",
        "type": "code",
        "location": "/q_transformer/q_robotic_transformer.py:508-545"
    },
    "129": {
        "file_id": 7,
        "content": "The code defines a class with member variables including RMSNorm, Dropout, and Linear layers. It also includes an optional Memory Key-Value (mem_kv) parameter for storing previously seen key-value pairs. The forward method takes in input tensor x, potentially context data, masks, and a conditional function. Assertion is used to ensure the context and self.context_norm exist simultaneously.",
        "type": "comment"
    },
    "130": {
        "file_id": 7,
        "content": "        if exists(context):\n            context = self.context_norm(context)\n        kv_input = default(context, x)\n        x = self.norm(x)\n        assert xnor(exists(cond_fn), self.adaptive_ln)\n        if exists(cond_fn):\n            x = cond_fn(x)\n        q, k, v = self.to_q(x), *self.to_kv(kv_input).chunk(2, dim = -1)\n        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), (q, k, v))\n        if exists(cache):\n            ck, cv = cache\n            k = torch.cat((ck, k), dim = -2)\n            v = torch.cat((cv, v), dim = -2)\n        new_kv_cache = torch.stack((k, v))\n        if exists(self.mem_kv):\n            mk, mv = map(lambda t: repeat(t, '... -> b ...', b = b), self.mem_kv)\n            k = torch.cat((mk, k), dim = -2)\n            v = torch.cat((mv, v), dim = -2)\n            if exists(mask):\n                mask = F.pad(mask, (self.num_mem_kv, 0), value = True)\n            if exists(attn_mask):\n                attn_mask = F.pad(attn_mask, (self.num_mem_kv, 0), value = True)",
        "type": "code",
        "location": "/q_transformer/q_robotic_transformer.py:547-580"
    },
    "131": {
        "file_id": 7,
        "content": "This code is responsible for processing input and output data in a transformer layer. It applies normalization, checks conditional functions, handles cache, and performs rearrangements of tensors. The main function seems to be the 'self.to_q' which splits the input into queries (q), keys (k), and values (v) for use in a transformer network. This code is part of a Q-Transformer model, hence the emphasis on 'q' in q_transformer.",
        "type": "comment"
    },
    "132": {
        "file_id": 7,
        "content": "        out = self.attend(q, k, v, mask = mask, attn_mask = attn_mask)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        out = self.to_out(out)\n        if not return_cache:\n            return out\n        return out, new_kv_cache\nclass Transformer(Module):\n    def __init__(\n        self,\n        dim,\n        dim_head = 64,\n        heads = 8,\n        depth = 6,\n        attn_dropout = 0.,\n        ff_dropout = 0.,\n        adaptive_ln = False,\n        flash_attn = True,\n        cross_attend = False,\n        causal = False,\n        final_norm = True\n    ):\n        super().__init__()\n        self.layers = ModuleList([])\n        attn_kwargs = dict(\n            dim = dim,\n            heads = heads,\n            dim_head = dim_head,\n            dropout = attn_dropout,\n            flash = flash_attn\n        )\n        for _ in range(depth):\n            self.layers.append(ModuleList([\n                TransformerAttention(**attn_kwargs, causal = causal, adaptive_ln = adaptive_ln, norm_context = False),\n                TransformerAttention(**attn_kwargs, norm_context = True) if cross_attend else None,",
        "type": "code",
        "location": "/q_transformer/q_robotic_transformer.py:582-621"
    },
    "133": {
        "file_id": 7,
        "content": "The code defines a Transformer class with layers for attention mechanisms, where each layer consists of two TransformerAttention instances. The attention mechanisms can be causal and/or cross-attend based on the parameters provided. The class also includes optional adaptive normalization, final normalization, and flash attention.",
        "type": "comment"
    },
    "134": {
        "file_id": 7,
        "content": "                FeedForward(dim = dim, dropout = ff_dropout, adaptive_ln = adaptive_ln)\n            ]))\n        self.norm = RMSNorm(dim) if final_norm else nn.Identity()\n    @beartype\n    def forward(\n        self,\n        x,\n        cond_fns: Optional[Tuple[Callable, ...]] = None,\n        attn_mask = None,\n        context: Optional[Tensor] = None,\n        cache: Optional[Tensor] = None,\n        return_cache = False\n    ):\n        has_cache = exists(cache)\n        if has_cache:\n            x_prev, x = x[..., :-1, :], x[..., -1:, :]\n        cond_fns = iter(default(cond_fns, []))\n        cache = iter(default(cache, []))\n        new_caches = []\n        for attn, maybe_cross_attn, ff in self.layers:\n            attn_out, new_cache = attn(\n                x,\n                attn_mask = attn_mask,\n                cond_fn = next(cond_fns, None),\n                return_cache = True,\n                cache = next(cache, None)\n            )\n            new_caches.append(new_cache)\n            x = x + attn_out\n            if exists(maybe_cross_attn):",
        "type": "code",
        "location": "/q_transformer/q_robotic_transformer.py:622-660"
    },
    "135": {
        "file_id": 7,
        "content": "This function defines a class with a forward method for transformer layers. It takes input x, optional conditioning functions, attention mask, context tensor, and cache. The function iterates through the transformer layers, applying each layer's attention operation to the input, accumulating outputs, and storing intermediate results in cache. The final output is the sum of all attention outputs.",
        "type": "comment"
    },
    "136": {
        "file_id": 7,
        "content": "                assert exists(context)\n                x = maybe_cross_attn(x, context = context) + x\n            x = ff(x, cond_fn = next(cond_fns, None)) + x\n        new_caches = torch.stack(new_caches)\n        if has_cache:\n            x = torch.cat((x_prev, x), dim = -2)\n        out = self.norm(x)\n        if not return_cache:\n            return out\n        return out, new_caches\n# token learner module\nclass TokenLearner(Module):\n    \"\"\"\n    https://arxiv.org/abs/2106.11297\n    using the 1.1 version with the MLP (2 dense layers with gelu) for generating attention map\n    \"\"\"\n    def __init__(\n        self,\n        *,\n        dim,\n        ff_mult = 2,\n        num_output_tokens = 8,\n        num_layers = 2\n    ):\n        super().__init__()\n        inner_dim = dim * ff_mult * num_output_tokens\n        self.num_output_tokens = num_output_tokens\n        self.net = nn.Sequential(\n            nn.Conv2d(dim * num_output_tokens, inner_dim, 1, groups = num_output_tokens),\n            nn.GELU(),\n            nn.Conv2d(inner_dim, num_output_tokens, 1, groups = num_output_tokens),",
        "type": "code",
        "location": "/q_transformer/q_robotic_transformer.py:661-701"
    },
    "137": {
        "file_id": 7,
        "content": "This code defines a transformer model for robotic tasks with a token learner module. The transformer model includes attention layers and feed-forward layers, with the option to return cached output if desired. The token learner module is responsible for generating an attention map using MLP (2 dense layers with GELU).",
        "type": "comment"
    },
    "138": {
        "file_id": 7,
        "content": "        )\n    def forward(self, x):\n        x, ps = pack_one(x, '* c h w')\n        x = repeat(x, 'b c h w -> b (g c) h w', g = self.num_output_tokens)\n        attn = self.net(x)\n        attn = rearrange(attn, 'b g h w -> b 1 g h w')\n        x = rearrange(x, 'b (g c) h w -> b c g h w', g = self.num_output_tokens)\n        x = reduce(x * attn, 'b c g h w -> b c g', 'mean')\n        x = unpack_one(x, ps, '* c n')\n        return x\n# Dueling heads for Q value\nclass DuelingHead(Module):\n    def __init__(\n        self,\n        dim,\n        expansion_factor = 2,\n        action_bins = 256\n    ):\n        super().__init__()\n        dim_hidden = dim * expansion_factor\n        self.stem = nn.Sequential(\n            nn.Linear(dim, dim_hidden),\n            nn.SiLU()\n        )\n        self.to_values = nn.Sequential(\n            nn.Linear(dim_hidden, 1)\n        )\n        self.to_advantages = nn.Sequential(\n            nn.Linear(dim_hidden, action_bins)\n        )\n    def forward(self, x):\n        x = self.stem(x)\n        advantages = self.to_advantages(x)",
        "type": "code",
        "location": "/q_transformer/q_robotic_transformer.py:702-744"
    },
    "139": {
        "file_id": 7,
        "content": "The code defines a Q-Transformer model for robotic tasks, which takes input and returns output based on attention mechanisms. It also includes a DuelingHead module for calculating Q values with separate value and advantage outputs.",
        "type": "comment"
    },
    "140": {
        "file_id": 7,
        "content": "        advantages = advantages - reduce(advantages, '... a -> ... 1', 'mean')\n        values = self.to_values(x)\n        q_values = values + advantages\n        return q_values.sigmoid()\n# Q head modules, for either single or multiple actions\nclass QHeadSingleAction(Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        num_learned_tokens = 8,\n        action_bins = 256,\n        dueling = False\n    ):\n        super().__init__()\n        self.action_bins = action_bins\n        if dueling:\n            self.to_q_values = nn.Sequential(\n                Reduce('b (f n) d -> b d', 'mean', n = num_learned_tokens),\n                DuelingHead(\n                    dim,\n                    action_bins = action_bins\n                )\n            )\n        else:\n            self.to_q_values = nn.Sequential(\n                Reduce('b (f n) d -> b d', 'mean', n = num_learned_tokens),\n                RMSNorm(dim),\n                nn.Linear(dim, action_bins),\n                nn.Sigmoid()\n            )\n    def get_random_actions(self, batch_size):",
        "type": "code",
        "location": "/q_transformer/q_robotic_transformer.py:745-782"
    },
    "141": {
        "file_id": 7,
        "content": "The code defines a class `QHeadSingleAction` for Q head modules in a neural network, used for either single or multiple actions. It utilizes a `Reduce` layer to reduce the dimensionality and applies a `DuelingHead` or additional layers depending on the dueling parameter. The function `get_random_actions` returns a list of random actions given a batch size.",
        "type": "comment"
    },
    "142": {
        "file_id": 7,
        "content": "        return torch.randint(0, self.action_bins, (batch_size,), device = self.device)\n    def get_optimal_actions(\n        self,\n        encoded_state,\n        return_q_values = False,\n        actions = None,\n        **kwargs\n    ):\n        assert not exists(actions), 'single actions will never receive previous actions'\n        q_values = self.forward(encoded_state)\n        max_q, action_indices = q_values.max(dim = -1)\n        if not return_q_values:\n            return action_indices\n        return action_indices, max_q\n    def forward(self, encoded_state):\n        return self.to_q_values(encoded_state)\nclass QHeadMultipleActions(Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        num_actions = 8,\n        action_bins = 256,\n        attn_depth = 2,\n        attn_dim_head = 32,\n        attn_heads = 8,\n        dueling = False\n    ):\n        super().__init__()\n        self.num_actions = num_actions\n        self.action_bins = action_bins\n        self.action_bin_embeddings = nn.Parameter(torch.zeros(num_actions, action_bins, dim))",
        "type": "code",
        "location": "/q_transformer/q_robotic_transformer.py:783-822"
    },
    "143": {
        "file_id": 7,
        "content": "The code defines a QHeadMultipleActions module for a robotic transformer model. It returns a batch of random actions, retrieves optimal actions from q-values, and has a forward function to convert encoded state into q-values. The QHeadMultipleActions class initializes with parameters such as number of actions, action bins, attention depth, etc.",
        "type": "comment"
    },
    "144": {
        "file_id": 7,
        "content": "        nn.init.normal_(self.action_bin_embeddings, std = 0.02)\n        self.transformer = Transformer(\n            dim = dim,\n            depth = attn_depth,\n            dim_head = attn_dim_head,\n            heads = attn_heads,\n            cross_attend = True,\n            adaptive_ln = False,\n            causal = True,\n            final_norm = True\n        )\n        self.final_norm = RMSNorm(dim)\n        self.dueling = dueling\n        if dueling:\n            self.to_values = nn.Parameter(torch.zeros(num_actions, dim))\n    @property\n    def device(self):\n        return self.action_bin_embeddings.device\n    def maybe_append_actions(self, sos_tokens, actions: Optional[Tensor] = None):\n        if not exists(actions):\n            return sos_tokens\n        batch, num_actions = actions.shape\n        action_embeddings = self.action_bin_embeddings[:num_actions]\n        action_embeddings = repeat(action_embeddings, 'n a d -> b n a d', b = batch)\n        past_action_bins = repeat(actions, 'b n -> b n 1 d', d = action_embeddings.shape[-1])",
        "type": "code",
        "location": "/q_transformer/q_robotic_transformer.py:823-854"
    },
    "145": {
        "file_id": 7,
        "content": "This code initializes the action_bin_embeddings with a normal distribution, creates a Transformer model, initializes RMSNorm layer and optionally initializes the to_values parameter for dueling. The device property returns the device used by the action_bin_embeddings, and maybe_append_actions appends actions if they exist else returns sos_tokens.",
        "type": "comment"
    },
    "146": {
        "file_id": 7,
        "content": "        bin_embeddings = action_embeddings.gather(-2, past_action_bins)\n        bin_embeddings = rearrange(bin_embeddings, 'b n 1 d -> b n d')\n        tokens, _ = pack((sos_tokens, bin_embeddings), 'b * d')\n        tokens = tokens[:, :self.num_actions] # last action bin not needed for the proposed q-learning\n        return tokens\n    def get_q_values(self, embed):\n        num_actions = embed.shape[-2]\n        action_bin_embeddings = self.action_bin_embeddings[:num_actions]\n        if self.dueling:\n            advantages = einsum('b n d, n a d -> b n a', embed, action_bin_embeddings)\n            values = einsum('b n d, n d -> b n', embed, self.to_values[:num_actions])\n            values = rearrange(values, 'b n -> b n 1')\n            q_values = values + (advantages - reduce(advantages, '... a -> ... 1', 'mean'))\n        else:\n            q_values = einsum('b n d, n a d -> b n a', embed, action_bin_embeddings)\n        return q_values.sigmoid()\n    def get_random_actions(self, batch_size, num_action_bins = None):",
        "type": "code",
        "location": "/q_transformer/q_robotic_transformer.py:856-879"
    },
    "147": {
        "file_id": 7,
        "content": "This code is implementing a Q-Transformer model for robotic tasks, including functions for gathering action embeddings and computing q-values. The get_q_values function uses the dueling architecture if self.dueling is True, otherwise it directly computes the q-values. The q-values are then returned after applying sigmoid activation. The get_random_actions function generates random actions for a given batch size and optional number of action bins.",
        "type": "comment"
    },
    "148": {
        "file_id": 7,
        "content": "        num_action_bins = default(num_action_bins, self.action_bins)\n        return torch.randint(0, num_action_bins, (batch_size, num_action_bins), device = self.device)\n    @torch.no_grad()\n    def get_optimal_actions(\n        self,\n        encoded_state,\n        return_q_values = False,\n        actions: Optional[Tensor] = None,\n        prob_random_action: float = 0.5,\n        **kwargs\n    ):\n        assert 0. <= prob_random_action <= 1.\n        batch = encoded_state.shape[0]\n        if prob_random_action == 1:\n            return self.get_random_actions(self, batch)\n        sos_token = reduce(encoded_state, 'b ... d -> b 1 d', 'mean')\n        tokens = self.maybe_append_actions(sos_token, actions = actions)\n        action_bins = []\n        cache = None\n        for action_idx in range(self.num_actions):\n            embed, cache = self.transformer(\n                tokens,\n                context = encoded_state,\n                cache = cache,\n                return_cache = True\n            )\n            last_embed = embed[:, action_idx]",
        "type": "code",
        "location": "/q_transformer/q_robotic_transformer.py:880-914"
    },
    "149": {
        "file_id": 7,
        "content": "This code defines a class for a robotic transformer, which can generate optimal actions based on encoded states. It includes methods for getting random actions when the probability of a random action is set to 1 and obtaining optimal actions by using a transformer model to predict the best course of action given an encoded state. The code also includes a method for maybe appending additional actions and handles caching for efficiency.",
        "type": "comment"
    },
    "150": {
        "file_id": 7,
        "content": "            bin_embeddings = self.action_bin_embeddings[action_idx]\n            q_values = einsum('b d, a d -> b a', last_embed, bin_embeddings)\n            selected_action_bins = q_values.argmax(dim = -1)\n            if prob_random_action > 0.:\n                random_mask = torch.zeros_like(selected_action_bins).float().uniform_(0., 1.) < prob_random_action\n                random_actions = self.get_random_actions(batch, 1)\n                random_actions = rearrange(random_actions, '... 1 -> ...')\n                selected_action_bins = torch.where(\n                    random_mask,\n                    random_actions,\n                    selected_action_bins\n                )\n            next_action_embed = bin_embeddings[selected_action_bins]\n            tokens, _ = pack((tokens, next_action_embed), 'b * d')\n            action_bins.append(selected_action_bins)\n        action_bins = torch.stack(action_bins, dim = -1)\n        if not return_q_values:\n            return action_bins\n        all_q_values = self.get_q_values(embed)",
        "type": "code",
        "location": "/q_transformer/q_robotic_transformer.py:915-944"
    },
    "151": {
        "file_id": 7,
        "content": "This code calculates Q-values for different actions based on the current state, and selects the action with the highest Q-value. If a random action is possible, it generates one with a certain probability and uses that instead. It then appends the selected action bins to a list, stacks them into a tensor, and returns either just the action bins or the Q-values depending on a flag.",
        "type": "comment"
    },
    "152": {
        "file_id": 7,
        "content": "        return action_bins, all_q_values\n    def forward(\n        self,\n        encoded_state: Tensor,\n        actions: Optional[Tensor] = None\n    ):\n        \"\"\"\n        einops\n        b - batch\n        n - number of actions\n        a - action bins\n        d - dimension\n        \"\"\"\n        # this is the scheme many hierarchical transformer papers do\n        sos_token = reduce(encoded_state, 'b ... d -> b 1 d', 'mean')\n        tokens = self.maybe_append_actions(sos_token, actions = actions)\n        embed = self.transformer(tokens, context = encoded_state)\n        return self.get_q_values(embed)\n# Robotic Transformer\nclass QRoboticTransformer(Module):\n    @beartype\n    def __init__(\n        self,\n        *,\n        vit: Union[Dict[str, Any], MaxViT],\n        num_actions = 8,\n        action_bins = 256,\n        depth = 6,\n        heads = 8,\n        dim_head = 64,\n        token_learner_ff_mult = 2,\n        token_learner_num_layers = 2,\n        token_learner_num_output_tokens = 8,\n        cond_drop_prob = 0.2,\n        use_attn_conditioner = False,",
        "type": "code",
        "location": "/q_transformer/q_robotic_transformer.py:945-988"
    },
    "153": {
        "file_id": 7,
        "content": "This code defines a QRoboticTransformer class that inherits from Module. It takes inputs encoded_state and optional actions, and returns action_bins and all_q_values. The forward function follows a common scheme, using SOS token, appending actions if provided, passing tokens through transformer for embeddings, then getting q-values with get_q_values.",
        "type": "comment"
    },
    "154": {
        "file_id": 7,
        "content": "        conditioner_kwargs: dict = dict(),\n        dueling = False,                       # https://arxiv.org/abs/1511.06581\n        flash_attn = True,\n        condition_on_text = True,\n        q_head_attn_kwargs: dict = dict(\n            attn_heads = 8,\n            attn_dim_head = 64,\n            attn_depth = 2\n        )\n    ):\n        super().__init__()\n        # vit\n        if isinstance(vit, dict):\n            vit = MaxViT(**vit)\n        self.vit = vit\n        self.num_vit_stages = len(vit.cond_hidden_dims)\n        attend_dim = vit.embed_dim\n        # q-transformer related action embeddings\n        assert num_actions >= 1\n        self.num_actions = num_actions\n        self.is_single_action = num_actions == 1\n        self.action_bins = action_bins\n        # conditioning\n        self.condition_on_text = condition_on_text\n        if condition_on_text:\n            conditioner_klass = AttentionTextConditioner if use_attn_conditioner else TextConditioner\n            self.conditioner = conditioner_klass(\n                hidden_dims = (*tuple(vit.cond_hidden_dims), *((attend_dim,) * depth * 2)),",
        "type": "code",
        "location": "/q_transformer/q_robotic_transformer.py:989-1028"
    },
    "155": {
        "file_id": 7,
        "content": "This code defines a class for the QRoboticTransformer model. It takes several parameters such as vit (a Vision Transformer), num_actions, action_bins, use_attn_conditioner, and others. It initializes the Vit model, checks if it's a dictionary and converts it to an instance of MaxViT if necessary. It also sets the number of ViT stages based on the number of hidden dimensions in ViT. The code then handles action embeddings and conditioning based on input parameters.",
        "type": "comment"
    },
    "156": {
        "file_id": 7,
        "content": "                hiddens_channel_first = (*((True,) * self.num_vit_stages), *((False,) * depth * 2)),\n                cond_drop_prob = cond_drop_prob,\n                **conditioner_kwargs\n            )\n        else:\n            self.conditioner = NullConditioner(hidden_dims = tuple())\n        self.token_learner = TokenLearner(\n            dim = vit.embed_dim,\n            ff_mult = token_learner_ff_mult,\n            num_output_tokens = token_learner_num_output_tokens,\n            num_layers = token_learner_num_layers\n        )\n        self.num_learned_tokens = token_learner_num_output_tokens\n        self.transformer_depth = depth\n        self.transformer = Transformer(\n            dim = attend_dim,\n            dim_head = dim_head,\n            heads = heads,\n            depth = depth,\n            flash_attn = flash_attn,\n            adaptive_ln = condition_on_text,\n            final_norm = True\n        )\n        self.cond_drop_prob = cond_drop_prob\n        # Q head\n        if self.is_single_action:\n            self.q_head = QHeadSingleAction(",
        "type": "code",
        "location": "/q_transformer/q_robotic_transformer.py:1029-1062"
    },
    "157": {
        "file_id": 7,
        "content": "This code is creating a Robotic Transformer model with configurable parameters. If the 'is_single_action' flag is True, it creates a QHeadSingleAction instance for a single-action scenario. Otherwise, it uses NullConditioner to handle multi-action scenarios. The model consists of a TokenLearner, which learns token representations, and a Transformer, responsible for attention mechanisms, with configurable dimensions, heads, depth, and adaptive normalization. The cond_drop_prob parameter determines the probability of dropping connections in conditioning layers.",
        "type": "comment"
    },
    "158": {
        "file_id": 7,
        "content": "                attend_dim,\n                num_learned_tokens = self.num_learned_tokens,\n                action_bins = action_bins,\n                dueling = dueling\n            )\n        else:\n            self.q_head = QHeadMultipleActions(\n                attend_dim,\n                action_bins = action_bins,\n                dueling = dueling,\n                **q_head_attn_kwargs\n            )\n    @property\n    def device(self):\n        return next(self.parameters()).device\n    def get_random_actions(self, batch_size = 1):\n        return self.q_head.get_random_actions(batch_size)\n    @beartype\n    def embed_texts(self, texts: List[str]):\n        return self.conditioner.embed_texts(texts)\n    @torch.no_grad()\n    def get_optimal_actions(\n        self,\n        *args,\n        return_q_values = False,\n        actions: Optional[Tensor] = None,\n        **kwargs\n    ):\n        encoded_state = self.encode_state(*args, **kwargs)\n        return self.q_head.get_optimal_actions(encoded_state, return_q_values = return_q_values, actions = actions)",
        "type": "code",
        "location": "/q_transformer/q_robotic_transformer.py:1063-1096"
    },
    "159": {
        "file_id": 7,
        "content": "This code defines a Q-transformer model for robotic tasks. It includes a `QHeadMultipleActions` for multi-action predictions and a `QHead` class for single action predictions. The model has methods for embedding texts, getting random actions, and finding optimal actions based on encoded state input.",
        "type": "comment"
    },
    "160": {
        "file_id": 7,
        "content": "    def get_actions(\n        self,\n        video,\n        *args,\n        prob_random_action = 0.,  # otherwise known as epsilon in RL\n        **kwargs,\n    ):\n        batch_size = video.shape[0]\n        assert 0. <= prob_random_action <= 1.\n        if random() < prob_random_action:\n            return self.get_random_actions(batch_size = batch_size)\n        return self.get_optimal_actions(video, *args, **kwargs)\n    def encode_state(\n        self,\n        video: Tensor,\n        texts: Optional[Union[List[str], Tuple[str]]] = None,\n        text_embeds: Optional[Tensor] = None,\n        actions: Optional[Tensor] = None,\n        cond_drop_prob = 0.,\n    ):\n        \"\"\"\n        einops\n        b - batch\n        c - channels\n        f - frames\n        h - height\n        w - width\n        n - number of learned tokens\n        \"\"\"\n        if not self.condition_on_text:\n            assert (not exists(texts) and not exists(text_embeds)), 'neither texts nor text embeds should be passed in'\n        else:\n            assert exists(te",
        "type": "code",
        "location": "/q_transformer/q_robotic_transformer.py:1098-1134"
    },
    "161": {
        "file_id": 7,
        "content": "The `get_actions` function in the code selects a random action with probability `prob_random_action`, otherwise it gets the optimal actions using `self.get_optimal_actions`. The `encode_state` function takes video, texts (optional), text embeds (optional), and actions (optional) as input, and encodes the state for a robotic transformer model.",
        "type": "comment"
    },
    "162": {
        "file_id": 7,
        "content": "xts) ^ exists(text_embeds), 'either texts or text embeds must be passed in if conditioning on instructions'\n        if exists(texts) and isinstance(texts, tuple):\n            texts = list(texts)\n        text_cond_kwargs = dict(texts = texts, text_embeds = text_embeds)\n        depth = self.transformer_depth\n        cond_drop_prob = default(cond_drop_prob, self.cond_drop_prob)\n        frames, device = video.shape[2], video.device\n        cond_fns, _ = self.conditioner(\n            **text_cond_kwargs,\n            cond_drop_prob = cond_drop_prob,\n            repeat_batch = (*((frames,) * self.num_vit_stages), *((1,) * self.transformer_depth * 2))\n        )\n        vit_cond_fns, transformer_cond_fns = cond_fns[:-(depth * 2)], cond_fns[-(depth * 2):]\n        video = rearrange(video, 'b c f h w -> b f c h w')\n        images, packed_shape = pack_one(video, '* c h w')\n        tokens = self.vit(\n            images,\n            texts = texts,\n            cond_fns = vit_cond_fns,\n            cond_drop_prob = cond_drop_prob,",
        "type": "code",
        "location": "/q_transformer/q_robotic_transformer.py:1134-1161"
    },
    "163": {
        "file_id": 7,
        "content": "This code conditionally processes either 'texts' or 'text_embeds' and applies them to the transformer model. It checks if either of these inputs exist, converts tuple to list if necessary, creates a dictionary with these kwargs, sets the depth and dropout probability, passes it to the conditioner function for video frames, separates the output functions for Vision Transformer (Vit) and Transformer stages, rearranges video shape, packs the image data, and finally feeds the images, texts, conditional functions, and dropout probability to the Vision Transformer model.",
        "type": "comment"
    },
    "164": {
        "file_id": 7,
        "content": "            return_embeddings = True\n        )\n        tokens = unpack_one(tokens, packed_shape, '* c h w')\n        learned_tokens = self.token_learner(tokens)\n        tokens_per_frame = learned_tokens.shape[-1]\n        learned_tokens = rearrange(learned_tokens, 'b f c n -> b (f n) c')\n        # causal attention mask\n        attn_mask = ~torch.ones((frames, frames), dtype = torch.bool, device = device).triu(1)\n        attn_mask = repeat(attn_mask, 'i j -> (i r1) (j r2)', r1 = self.num_learned_tokens, r2 = self.num_learned_tokens)\n        # sinusoidal positional embedding\n        pos_emb = posemb_sincos_1d(frames, learned_tokens.shape[-1], dtype = learned_tokens.dtype, device = learned_tokens.device)\n        learned_tokens = learned_tokens + repeat(pos_emb, 'n d -> (n r) d', r = self.num_learned_tokens)\n        # attention\n        attended_tokens = self.transformer(learned_tokens, cond_fns = transformer_cond_fns, attn_mask = attn_mask)\n        return attended_tokens\n    @classifier_free_guidance\n    def forward(",
        "type": "code",
        "location": "/q_transformer/q_robotic_transformer.py:1162-1189"
    },
    "165": {
        "file_id": 7,
        "content": "This code is part of a Transformer-based model for robotic manipulation tasks. It unpacks the input tokens, learns new tokens from the token learner, rearranges them to fit the model's structure, and applies a causal attention mask to avoid attending to future frames. It also adds sinusoidal positional embeddings and passes the data through a Transformer for multi-headed self-attention. The code is annotated with classifier-free guidance for generating diverse outputs.",
        "type": "comment"
    },
    "166": {
        "file_id": 7,
        "content": "        self,\n        video: Tensor,\n        texts: Optional[List[str]] = None,\n        text_embeds: Optional[Tensor] = None,\n        actions: Optional[Tensor] = None,\n        cond_drop_prob = 0.,\n    ):\n        encoded_state = self.encode_state(\n            video = video,\n            texts = texts,\n            text_embeds = text_embeds,\n            actions = actions,\n            cond_drop_prob = cond_drop_prob\n        )\n        # head that returns the q values\n        # supporting both single and multiple actions\n        if self.is_single_action:\n            assert not exists(actions), 'actions should not be passed in for single action robotic transformer'\n            q_values = self.q_head(encoded_state)\n        else:\n            q_values = self.q_head(encoded_state, actions = actions)\n        return q_values",
        "type": "code",
        "location": "/q_transformer/q_robotic_transformer.py:1190-1216"
    },
    "167": {
        "file_id": 7,
        "content": "The code defines a method that takes in video, texts (optional), text embeds (optional), actions (optional), and cond_drop_prob as input. It encodes the state using the 'encode\\_state' function and then uses a head to calculate q values depending on whether it's a single or multiple action robotic transformer.",
        "type": "comment"
    },
    "168": {
        "file_id": 8,
        "content": "/setup.py",
        "type": "filepath"
    },
    "169": {
        "file_id": 8,
        "content": "This code is a setup file for a Python package named \"q-transformer\". It specifies the package's name, version, author, description, and dependencies. The file also sets up the install requirements and classifiers for the package.",
        "type": "summary"
    },
    "170": {
        "file_id": 8,
        "content": "from setuptools import setup, find_packages\nsetup(\n  name = 'q-transformer',\n  packages = find_packages(exclude=[]),\n  version = '0.1.5',\n  license='MIT',\n  description = 'Q-Transformer',\n  author = 'Phil Wang',\n  author_email = 'lucidrains@gmail.com',\n  long_description_content_type = 'text/markdown',\n  url = 'https://github.com/lucidrains/q-transformer',\n  keywords = [\n    'artificial intelligence',\n    'deep learning',\n    'attention mechanisms',\n    'transformers',\n    'q-learning'\n  ],\n  install_requires=[\n    'accelerate',\n    'beartype',\n    'classifier-free-guidance-pytorch>=0.4.2',\n    'einops>=0.7.0',\n    'ema-pytorch>=0.3.1',\n    'numpy',\n    'torchtyping',\n    'torch>=2.0'\n  ],\n  classifiers=[\n    'Development Status :: 4 - Beta',\n    'Intended Audience :: Developers',\n    'Topic :: Scientific/Engineering :: Artificial Intelligence',\n    'License :: OSI Approved :: MIT License',\n    'Programming Language :: Python :: 3.6',\n  ],\n)",
        "type": "code",
        "location": "/setup.py:1-37"
    },
    "171": {
        "file_id": 8,
        "content": "This code is a setup file for a Python package named \"q-transformer\". It specifies the package's name, version, author, description, and dependencies. The file also sets up the install requirements and classifiers for the package.",
        "type": "comment"
    }
}