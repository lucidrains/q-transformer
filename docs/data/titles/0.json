{
    "/README.md": "Q-Transformer: Scalable Offline RL Model",
    "/README.md:1-44": "Q-Transformer Package Installation and Usage",
    "/README.md:111-125": "Encoder-Decoder Model with Attention and Gating",
    "/README.md:127-138": "Q-Transformer: Enhancements and Experiments",
    "/README.md:139-152": "BibTeX Entries for Publications",
    "/README.md:45-86": "Mock Environment and QLearner Integration",
    "/README.md:88-111": "Video-Based Model: Optimal Actions from Instructions",
    "/q_transformer/__init__.py": "Transformer-Based RL Architecture",
    "/q_transformer/agent.py": "Q-Transformer Agent Training: Reinforcement Learning Experience",
    "/q_transformer/agent.py:1-48": "Replay Memory Dataset Class Creation",
    "/q_transformer/agent.py:101-129": "BaseEnvironment Class Definition",
    "/q_transformer/agent.py:131-168": "Agent Initializes with QRoboticTransformer",
    "/q_transformer/agent.py:170-197": "Q-Transformer Agent Initialization",
    "/q_transformer/agent.py:198-214": "Epsilon Scheduling in Q-Transformer Agent",
    "/q_transformer/agent.py:216-248": "Q-Transformer Agent Interactions",
    "/q_transformer/agent.py:249-274": "Reinforcement Learning Agent Memory Storage",
    "/q_transformer/agent.py:49-72": "Agent Attribute Initialization",
    "/q_transformer/agent.py:74-99": "Training Episodes Based on Timesteps",
    "/q_transformer/attend.py": "Attention-Based PyTorch Network Operations",
    "/q_transformer/attend.py:1-58": "Attention-based Neural Network Module",
    "/q_transformer/attend.py:124-145": "Soft Attention Masking",
    "/q_transformer/attend.py:59-86": "Flash Attention Dropout Self-Attention Class",
    "/q_transformer/attend.py:87-123": "Scaled Dot Product Attention with Causality",
    "/q_transformer/mocks.py": "Q-Transformer Mock Environments",
    "/q_transformer/mocks.py:1-38": "Mock Environment for Q-Transformer Model",
    "/q_transformer/mocks.py:39-73": "MockReplayNStepDataset: Custom Dataset for RL Agents Training",
    "/q_transformer/mocks.py:74-91": "Custom Dataloader for RL Tasks",
    "/q_transformer/optimizer.py": "Weight Decay Optimizer Functions",
    "/q_transformer/q_learner.py": "Robotic Q-Learner for Transformers",
    "/q_transformer/q_learner.py:1-47": "Q-Learner: QIntermediates and Losses",
    "/q_transformer/q_learner.py:128-169": "Initializing Q Learner Components and Accelerator",
    "/q_transformer/q_learner.py:170-210": "QTransformer Model Initialization and Training Setup",
    "/q_transformer/q_learner.py:211-248": "Q Learner Class Methods",
    "/q_transformer/q_learner.py:250-274": "Q-Learner Class and Q Transformer",
    "/q_transformer/q_learner.py:275-297": "Q-Learning Algorithm: Smooth Model, Bellman Equation, MSE Loss",
    "/q_transformer/q_learner.py:299-334": "N-Step Q-Learning on Embedding Data",
    "/q_transformer/q_learner.py:336-366": "Discounted Q-value Calculation and Learning",
    "/q_transformer/q_learner.py:368-403": "Autoregressive Q-Learner Function",
    "/q_transformer/q_learner.py:405-438": "Update Q-values with Monte Carlo Method",
    "/q_transformer/q_learner.py:440-470": "Q-Learning Text Embeddings Algorithm",
    "/q_transformer/q_learner.py:472-495": "Q-Value Calculation and Clamping",
    "/q_transformer/q_learner.py:49-92": "QLearner for Robotic Transformers: Reinforcement Learning and Rewards",
    "/q_transformer/q_learner.py:497-526": "Q-Learner: Multi-Step Q-Learning Implementation",
    "/q_transformer/q_learner.py:528-555": "Conservative Regularized Q-Learning Algorithm",
    "/q_transformer/q_learner.py:557-593": "QLearner: Combining TD Loss and Conservative Regularization",
    "/q_transformer/q_learner.py:594-631": "Asynchronous Sync-Free Q Learner Training",
    "/q_transformer/q_learner.py:632-637": "Training Completes Every Checkpoint",
    "/q_transformer/q_learner.py:93-126": "Q-Learner Class in Reinforcement Learning",
    "/q_transformer/q_robotic_transformer.py": "Q-Transformer for Robotic Actions",
    "/q_transformer/q_robotic_transformer.py:1-44": "Utility Functions for Robotic Transformers",
    "/q_transformer/q_robotic_transformer.py:1029-1062": "Robotic Transformer Model: QHead & Adaptive Normalization",
    "/q_transformer/q_robotic_transformer.py:1063-1096": "Q-Transformer Robotic Model with Multi-Action Predictions",
    "/q_transformer/q_robotic_transformer.py:1098-1134": "Robotic Transformer: Action Selection and State Encoding",
    "/q_transformer/q_robotic_transformer.py:1134-1161": "Conditional Vision Transformer for Robotics",
    "/q_transformer/q_robotic_transformer.py:116-155": "Posemb Sincos Function",
    "/q_transformer/q_robotic_transformer.py:1162-1189": "Token Unpacking and Transformer Attention",
    "/q_transformer/q_robotic_transformer.py:1190-1216": "Robotic Transformer Q-values Calculation",
    "/q_transformer/q_robotic_transformer.py:156-198": "MBConv Residual Block for CNNs",
    "/q_transformer/q_robotic_transformer.py:200-242": "Robotic Transformer with DropSampling and MBConv Block",
    "/q_transformer/q_robotic_transformer.py:243-288": "Residual Convolutional Attention Network",
    "/q_transformer/q_robotic_transformer.py:289-332": "Multi-Head QTransformer with Gate",
    "/q_transformer/q_robotic_transformer.py:334-374": "MaxViT Initialization in Robotic Transformer",
    "/q_transformer/q_robotic_transformer.py:376-405": "MaxViT Module Initialization",
    "/q_transformer/q_robotic_transformer.py:406-418": "Multi-Operation Neural Layer with Q-Transformer",
    "/q_transformer/q_robotic_transformer.py:419-462": "Robotic Q-Transformer Model",
    "/q_transformer/q_robotic_transformer.py:46-78": "Rotary Embeddings for 2D Positions",
    "/q_transformer/q_robotic_transformer.py:463-507": "Robotic Transformer with Window and Grid Attention",
    "/q_transformer/q_robotic_transformer.py:508-545": "Memory-Aware Q Robotic Transformer",
    "/q_transformer/q_robotic_transformer.py:547-580": "Q-Transformer: Query Processing Layer",
    "/q_transformer/q_robotic_transformer.py:582-621": "Customizable Transformer Attention Layers",
    "/q_transformer/q_robotic_transformer.py:622-660": "Transformer Layer Forward Pass",
    "/q_transformer/q_robotic_transformer.py:661-701": "Robotic Attention Transformer Model",
    "/q_transformer/q_robotic_transformer.py:702-744": "Robotic Q-Transformer Model with Dueling Head",
    "/q_transformer/q_robotic_transformer.py:745-782": "QHeadSingleAction Class for Neural Networks",
    "/q_transformer/q_robotic_transformer.py:783-822": "QHeadMultipleActions: Robotic Transformer Q-Values",
    "/q_transformer/q_robotic_transformer.py:79-114": "Batch Normalization Functions and Layers",
    "/q_transformer/q_robotic_transformer.py:823-854": "Initializing Q Robotic Transformer Model",
    "/q_transformer/q_robotic_transformer.py:856-879": "Random Action Generator",
    "/q_transformer/q_robotic_transformer.py:880-914": "Efficient Robotic Transformer for Optimal Actions",
    "/q_transformer/q_robotic_transformer.py:915-944": "Q-Value Calculation for Robotic Transformer Actions",
    "/q_transformer/q_robotic_transformer.py:945-988": "QRoboticTransformer: Q-Value Prediction for Robotic Tasks",
    "/q_transformer/q_robotic_transformer.py:989-1028": "Robotic Vision Transformer Model",
    "/setup.py": "Setting Up Q-Transformer"
}