{
    "summary": "QLearner class for reinforcement learning on robotic transformer models, utilizing named tuples and initializing components, uses QTransformer model with training parameters for Q-learning, includes autoregressive_q_learn function for updating Q-values and supports various Q-learning types. Trains using accelerator and saves checkpoints periodically.",
    "details": [
        {
            "comment": "This code imports various libraries and defines a named tuple called QIntermediates and another one called Losses. It also provides a helper function called exists() to check if a value is None or not. The code appears to be part of a larger module for Reinforcement Learning, specifically using the Quantile Regression method. The named tuples are likely used to store intermediate results from the Q-learning algorithm and losses calculated during training.",
            "location": "\"/media/root/Prima/works/q-transformer/docs/src/q_transformer/q_learner.py\":0-46",
            "content": "from pathlib import Path\nfrom functools import partial\nfrom contextlib import nullcontext\nfrom collections import namedtuple\nimport torch\nimport torch.nn.functional as F\nimport torch.distributed as dist\nfrom torch import nn, einsum, Tensor\nfrom torch.nn import Module, ModuleList\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchtyping import TensorType\nfrom einops import rearrange, repeat, pack, unpack\nfrom einops.layers.torch import Rearrange\nfrom beartype import beartype\nfrom beartype.typing import Optional, Union, List, Tuple\nfrom q_transformer.q_robotic_transformer import QRoboticTransformer\nfrom q_transformer.optimizer import get_adam_optimizer\nfrom accelerate import Accelerator\nfrom accelerate.utils import DistributedDataParallelKwargs\nfrom ema_pytorch import EMA\n# constants\nQIntermediates = namedtuple('QIntermediates', [\n    'q_pred_all_actions',\n    'q_pred',\n    'q_next',\n    'q_target'\n])\nLosses = namedtuple('Losses', [\n    'td_loss',\n    'conservative_reg_loss'\n])\n# helpers\ndef exists(val):\n    return val is not None"
        },
        {
            "comment": "This code defines a QLearner class for reinforcement learning on robotic transformer models. It includes functions for batch selection, gradient accumulation, and Monte Carlo returns. The QLearner class takes in a model (QRoboticTransformer or Module), dataset, batch size, number of training steps, learning rate, minimum reward, grad accumulation frequency, optional Monte Carlo return, weight decay, accelerator, and dataloader kwargs as parameters.",
            "location": "\"/media/root/Prima/works/q-transformer/docs/src/q_transformer/q_learner.py\":48-91",
            "content": "def default(val, d):\n    return val if exists(val) else d\ndef is_divisible(num, den):\n    return (num % den) == 0\ndef pack_one(t, pattern):\n    return pack([t], pattern)\ndef unpack_one(t, ps, pattern):\n    return unpack(t, ps, pattern)[0]\ndef cycle(dl):\n    while True:\n        for batch in dl:\n            yield batch\n# tensor helpers\ndef batch_select_indices(t, indices):\n    indices = rearrange(indices, '... -> ... 1')\n    selected = t.gather(-1, indices)\n    return rearrange(selected, '... 1 -> ...')\n# Q learning on robotic transformer\nclass QLearner(Module):\n    @beartype\n    def __init__(\n        self,\n        model: Union[QRoboticTransformer, Module],\n        *,\n        dataset: Dataset,\n        batch_size: int,\n        num_train_steps: int,\n        learning_rate: float,\n        min_reward: float = 0.,\n        grad_accum_every: int = 1,\n        monte_carlo_return: Optional[float] = None,\n        weight_decay: float = 0.,\n        accelerator: Optional[Accelerator] = None,\n        accelerator_kwargs: dict = dict(),\n        dataloader_kwargs: dict = dict("
        },
        {
            "comment": "This code snippet defines a class for a Q-learner in a reinforcement learning algorithm. It takes various hyperparameters such as the discount factor, whether to use n-step Q-learning, and conservative regularization loss weight. The class also initializes an online (evaluated) Q model and an ema (target) Q model.",
            "location": "\"/media/root/Prima/works/q-transformer/docs/src/q_transformer/q_learner.py\":92-125",
            "content": "            shuffle = True\n        ),\n        q_target_ema_kwargs: dict = dict(\n            beta = 0.99,\n            update_after_step = 10,\n            update_every = 5\n        ),\n        max_grad_norm = 0.5,\n        n_step_q_learning = False,\n        discount_factor_gamma = 0.98,\n        conservative_reg_loss_weight = 1., # they claim 1. is best in paper\n        optimizer_kwargs: dict = dict(),\n        checkpoint_folder = './checkpoints',\n        checkpoint_every = 1000,\n    ):\n        super().__init__()\n        self.is_multiple_actions = model.num_actions > 1\n        # q-learning related hyperparameters\n        self.discount_factor_gamma = discount_factor_gamma\n        self.n_step_q_learning = n_step_q_learning\n        self.has_conservative_reg_loss = conservative_reg_loss_weight > 0.\n        self.conservative_reg_loss_weight = conservative_reg_loss_weight\n        self.register_buffer('discount_matrix', None, persistent = False)\n        # online (evaluated) Q model\n        self.model = model\n        # ema (target) Q model"
        },
        {
            "comment": "The code initializes model-related components and accelerator for a Q learner. It creates an exponential moving average (EMA) model, sets the maximum gradient norm, gets an optimizer, initializes an accelerator with distributed data parallelism, sets minimum reward and Monte Carlo return settings, and prepares the dataloader. The components are then prepared using the provided accelerator.",
            "location": "\"/media/root/Prima/works/q-transformer/docs/src/q_transformer/q_learner.py\":127-168",
            "content": "        self.ema_model = EMA(\n            model,\n            include_online_model = False,\n            **q_target_ema_kwargs\n        )\n        self.max_grad_norm = max_grad_norm\n        self.optimizer = get_adam_optimizer(\n            model.parameters(),\n            lr = learning_rate,\n            wd = weight_decay,\n            **optimizer_kwargs\n        )\n        if not exists(accelerator):\n            accelerator = Accelerator(\n                kwargs_handlers = [\n                    DistributedDataParallelKwargs(find_unused_parameters = True)\n                ],\n                **accelerator_kwargs\n            )\n        self.accelerator = accelerator\n        self.min_reward = min_reward\n        self.monte_carlo_return = monte_carlo_return\n        self.dataloader = DataLoader(\n            dataset,\n            batch_size = batch_size,\n            **dataloader_kwargs\n        )\n        # prepare\n        (\n            self.model,\n            self.ema_model,\n            self.optimizer,\n            self.dataloader\n        ) = self.accelerator.prepare("
        },
        {
            "comment": "This code initializes a QTransformer model and sets up training parameters. It also creates an empty buffer 'zero' for loss calculation, and registers a step counter. The save method is used to store checkpoints of the model's state dictionary.",
            "location": "\"/media/root/Prima/works/q-transformer/docs/src/q_transformer/q_learner.py\":169-209",
            "content": "            self.model,\n            self.ema_model,\n            self.optimizer,\n            self.dataloader\n        )\n        # checkpointing related\n        self.checkpoint_every = checkpoint_every\n        self.checkpoint_folder = Path(checkpoint_folder)\n        self.checkpoint_folder.mkdir(exist_ok = True, parents = True)\n        assert self.checkpoint_folder.is_dir()\n        # dummy loss\n        self.register_buffer('zero', torch.tensor(0.))\n        # training step related\n        self.num_train_steps = num_train_steps\n        self.grad_accum_every = grad_accum_every\n        self.register_buffer('step', torch.tensor(0))\n    def save(\n        self,\n        checkpoint_num = None,\n        overwrite = True\n    ):\n        name = 'checkpoint'\n        if exists(checkpoint_num):\n            name += f'-{checkpoint_num}'\n        path = self.checkpoint_folder / (name + '.pt')\n        assert overwrite or not path.exists()\n        pkg = dict(\n            model = self.unwrap(self.model).state_dict(),\n            ema_model = self.unwrap(self.ema_model).state_dict(),"
        },
        {
            "comment": "The code defines a class with methods for saving and loading model state, getting the device used for training, checking if it's the main process, unwrapping models, printing messages, and waiting for all processes to finish. The class also provides a method to get the discount matrix based on timesteps.",
            "location": "\"/media/root/Prima/works/q-transformer/docs/src/q_transformer/q_learner.py\":210-247",
            "content": "            optimizer = self.optimizer.state_dict(),\n            step = self.step.item()\n        )\n        torch.save(pkg, str(path))\n    def load(self, path):\n        path = Path(path)\n        assert exists(path)\n        pkg = torch.load(str(path))\n        self.unwrap(self.model).load_state_dict(pkg['model'])\n        self.unwrap(self.ema_model).load_state_dict(pkg['ema_model'])\n        self.optimizer.load_state_dict(pkg['optimizer'])\n        self.step.copy_(pkg['step'])\n    @property\n    def device(self):\n        return self.accelerator.device\n    @property\n    def is_main(self):\n        return self.accelerator.is_main_process\n    def unwrap(self, module):\n        return self.accelerator.unwrap_model(module)\n    def print(self, msg):\n        return self.accelerator.print(msg)\n    def wait(self):\n        return self.accelerator.wait_for_everyone()\n    def get_discount_matrix(self, timestep):\n        if exists(self.discount_matrix) and self.discount_matrix.shape[-1] >= timestep:\n            return self.discount_matrix[:timestep, :timestep]"
        },
        {
            "comment": "The code defines the `q_learner` class that computes a discount matrix and performs Q-learning by taking input tensors of text embeddings, states, actions, next states, rewards, and done status. It uses the discount factor gamma (\u03b3) and calculates not_terminal values by inverting the done tensor. The code then predicts with an online Q transformer for reinforcement learning.",
            "location": "\"/media/root/Prima/works/q-transformer/docs/src/q_transformer/q_learner.py\":249-273",
            "content": "        timestep_arange = torch.arange(timestep, device = self.accelerator.device)\n        powers = (timestep_arange[None, :] - timestep_arange[:, None])\n        discount_matrix = torch.triu(self.discount_factor_gamma ** powers)\n        self.register_buffer('discount_matrix', discount_matrix, persistent = False)\n        return self.discount_matrix\n    def q_learn(\n        self,\n        text_embeds:    TensorType['b', 'd', float],\n        states:         TensorType['b', 'c', 'f', 'h', 'w', float],\n        actions:        TensorType['b', int],\n        next_states:    TensorType['b', 'c', 'f', 'h', 'w', float],\n        reward:         TensorType['b', float],\n        done:           TensorType['b', bool],\n        *,\n        monte_carlo_return = None\n    ) -> Tuple[TensorType[()], QIntermediates]:\n        # 'next' stands for the very next time step (whether state, q, actions etc)\n        \u03b3 = self.discount_factor_gamma\n        not_terminal = (~done).float()\n        # first make a prediction with online q robotic transformer"
        },
        {
            "comment": "This code implements the core Q-learning algorithm, selecting the Q-values for taken actions, using an exponentially smoothed model for future Q targets, applying Bellman's equation, and minimizing loss via mean squared error. It returns the loss and relevant intermediates for logging purposes.",
            "location": "\"/media/root/Prima/works/q-transformer/docs/src/q_transformer/q_learner.py\":274-296",
            "content": "        # select out the q-values for the action that was taken\n        q_pred_all_actions = self.model(states, text_embeds = text_embeds)\n        q_pred = batch_select_indices(q_pred_all_actions, actions)\n        # use an exponentially smoothed copy of model for the future q target. more stable than setting q_target to q_eval after each batch\n        # the max Q value is taken as the optimal action is implicitly the one with the highest Q score\n        q_next = self.ema_model(next_states, text_embeds = text_embeds).amax(dim = -1)\n        q_next.clamp_(min = default(monte_carlo_return, -1e4))\n        # Bellman's equation. most important line of code, hopefully done correctly\n        q_target = reward + not_terminal * (\u03b3 * q_next)\n        # now just force the online model to be able to predict this target\n        loss = F.mse_loss(q_pred, q_target)\n        # that's it. ~5 loc for the heart of q-learning\n        # return loss and some of the intermediates for logging\n        return loss, QIntermediates(q_pred_all_actions, q_pred, q_next, q_target)"
        },
        {
            "comment": "This function performs n-step Q-learning on a batch of data, taking in embedding vectors and state arrays. It packs the state arrays into a single array, repeats text embeddings per timestep, and then continues with further computations.",
            "location": "\"/media/root/Prima/works/q-transformer/docs/src/q_transformer/q_learner.py\":298-333",
            "content": "    def n_step_q_learn(\n        self,\n        text_embeds:    TensorType['b', 'd', float],\n        states:         TensorType['b', 't', 'c', 'f', 'h', 'w', float],\n        actions:        TensorType['b', 't', int],\n        next_states:    TensorType['b', 'c', 'f', 'h', 'w', float],\n        rewards:        TensorType['b', 't', float],\n        dones:          TensorType['b', 't', bool],\n        *,\n        monte_carlo_return = None\n    ) -> Tuple[TensorType[()], QIntermediates]:\n        \"\"\"\n        einops\n        b - batch\n        c - channels\n        f - frames\n        h - height\n        w - width\n        t - timesteps\n        a - action bins\n        q - q values\n        d - text cond dimension\n        \"\"\"\n        num_timesteps, device = states.shape[1], states.device\n        # fold time steps into batch\n        states, time_ps = pack_one(states, '* c f h w')\n        text_embeds, _ = pack_one(text_embeds, '* d')\n        # repeat text embeds per timestep\n        repeated_text_embeds = repeat(text_embeds, 'b ... -> (b n) ...', n = num_timesteps)"
        },
        {
            "comment": "The code calculates Q-values for each action in a batch and then selects the corresponding Q-value for the current action. It also applies discounting using the discount matrix, and the model learns to predict Q-target values for non-terminal states.",
            "location": "\"/media/root/Prima/works/q-transformer/docs/src/q_transformer/q_learner.py\":335-365",
            "content": "        \u03b3 = self.discount_factor_gamma\n        # anything after the first done flag will be considered terminal\n        dones = dones.cumsum(dim = -1) > 0\n        dones = F.pad(dones, (1, 0), value = False)\n        not_terminal = (~dones).float()\n        # get q predictions\n        actions = rearrange(actions, 'b t -> (b t)')\n        q_pred_all_actions = self.model(states, text_embeds = repeated_text_embeds)\n        q_pred = batch_select_indices(q_pred_all_actions, actions)\n        q_pred = unpack_one(q_pred, time_ps, '*')\n        q_next = self.ema_model(next_states, text_embeds = text_embeds).amax(dim = -1)\n        q_next.clamp_(min = default(monte_carlo_return, -1e4))\n        # prepare rewards and discount factors across timesteps\n        rewards, _ = pack([rewards, q_next], 'b *')\n        \u03b3 = self.get_discount_matrix(num_timesteps + 1)[:-1, :]\n        # account for discounting using the discount matrix\n        q_target = einsum('b t, q t -> b q', not_terminal * rewards, \u03b3)\n        # have transformer learn to predict above Q target"
        },
        {
            "comment": "This code defines a function for the Q-learner model in the q_transformer package. It calculates the loss between predicted Q values (q_pred) and target Q values (q_target). The function prepares the Q prediction by unpacking it, and returns both the loss and an object containing all intermediate Q predictions. Additionally, there is a separate function called `autoregressive_q_learn_handle_single_timestep` which handles single-step timesteps and calls the more general `autoregressive_q_learn` function for other cases. The code also performs array reshaping to handle different input dimensions.",
            "location": "\"/media/root/Prima/works/q-transformer/docs/src/q_transformer/q_learner.py\":367-402",
            "content": "        loss = F.mse_loss(q_pred, q_target)\n        # prepare q prediction\n        q_pred_all_actions = unpack_one(q_pred_all_actions, time_ps, '* a')\n        return loss, QIntermediates(q_pred_all_actions, q_pred, q_next, q_target)\n    def autoregressive_q_learn_handle_single_timestep(\n        self,\n        text_embeds,\n        states,\n        actions,\n        next_states,\n        rewards,\n        dones,\n        *,\n        monte_carlo_return = None\n    ):\n        \"\"\"\n        simply detect and handle single timestep\n        and use `autoregressive_q_learn` as more general function\n        \"\"\"\n        if states.ndim == 5:\n            states = rearrange(states, 'b ... -> b 1 ...')\n        if actions.ndim == 2:\n            actions = rearrange(actions, 'b ... -> b 1 ...')\n        if rewards.ndim == 1:\n            rewards = rearrange(rewards, 'b -> b 1')\n        if dones.ndim == 1:\n            dones = rearrange(dones, 'b -> b 1')\n        return self.autoregressive_q_learn(text_embeds, states, actions, next_states, rewards, dones, monte_carlo_return = monte_carlo_return)"
        },
        {
            "comment": "This function, `autoregressive_q_learn`, performs a type of reinforcement learning called \"Monte Carlo\" method to update the Q-values (a.k.a quality values) of an agent's actions in a given state based on observed rewards and terminal signals from the environment. It takes in tensors for text embeddings, states, actions, next states, rewards, and dones. The function returns a tuple consisting of a tensor for the updated Q-values (q), and intermediates related to the computation process.",
            "location": "\"/media/root/Prima/works/q-transformer/docs/src/q_transformer/q_learner.py\":404-437",
            "content": "    def autoregressive_q_learn(\n        self,\n        text_embeds:    TensorType['b', 'd', float],\n        states:         TensorType['b', 't', 'c', 'f', 'h', 'w', float],\n        actions:        TensorType['b', 't', 'n', int],\n        next_states:    TensorType['b', 'c', 'f', 'h', 'w', float],\n        rewards:        TensorType['b', 't', float],\n        dones:          TensorType['b', 't', bool],\n        *,\n        monte_carlo_return = None\n    ) -> Tuple[TensorType[()], QIntermediates]:\n        \"\"\"\n        einops\n        b - batch\n        c - channels\n        f - frames\n        h - height\n        w - width\n        t - timesteps\n        n - number of actions\n        a - action bins\n        q - q values\n        d - text cond dimension\n        \"\"\"\n        monte_carlo_return = default(monte_carlo_return, -1e4)\n        num_timesteps, device = states.shape[1], states.device\n        # fold time steps into batch\n        states, time_ps = pack_one(states, '* c f h w')\n        actions, _ = pack_one(actions, '* n')\n        text_embeds, _ = pack_one(text_embeds, '* d')"
        },
        {
            "comment": "This code is implementing a Q-learning algorithm for text-based tasks. It repeats the text embeddings per timestep, identifies non-terminal steps, scales rewards accordingly, predicts Q values for each action using a model, and calculates q_next by taking the maximum value of the next state's predictions from an ema_model. The code also clamps q_next to be greater than or equal to the monte_carlo_return value.",
            "location": "\"/media/root/Prima/works/q-transformer/docs/src/q_transformer/q_learner.py\":439-469",
            "content": "        # repeat text embeds per timestep\n        repeated_text_embeds = repeat(text_embeds, 'b ... -> (b n) ...', n = num_timesteps)\n        # anything after the first done flag will be considered terminal\n        dones = dones.cumsum(dim = -1) > 0\n        dones = F.pad(dones, (1, -1), value = False)\n        not_terminal = (~dones).float()\n        # rewards should not be given on and after terminal step\n        rewards = rewards * not_terminal\n        # because greek unicode is nice to look at\n        \u03b3 = self.discount_factor_gamma\n        # get predicted Q for each action\n        # unpack back to (b, t, n)\n        q_pred_all_actions = self.model(states, text_embeds = repeated_text_embeds, actions = actions)\n        q_pred = batch_select_indices(q_pred_all_actions, actions)\n        q_pred = unpack_one(q_pred, time_ps, '* n')\n        # get q_next\n        q_next = self.ema_model(next_states, text_embeds = text_embeds)\n        q_next = q_next.max(dim = -1).values\n        q_next.clamp_(min = monte_carlo_return)"
        },
        {
            "comment": "This code calculates the target Q values using an exponential moving average model, then clamps them to avoid negative values. It then separates the last action's target from the rest and calculates losses for all actions except the last one using mean squared error loss. Finally, it handles the last action separately by incorporating rewards and discounting future Q targets.",
            "location": "\"/media/root/Prima/works/q-transformer/docs/src/q_transformer/q_learner.py\":471-494",
            "content": "        # get target Q\n        # unpack back to - (b, t, n)\n        q_target_all_actions = self.ema_model(states, text_embeds = repeated_text_embeds, actions = actions)\n        q_target = q_target_all_actions.max(dim = -1).values\n        q_target.clamp_(min = monte_carlo_return)\n        q_target = unpack_one(q_target, time_ps, '* n')\n        # main contribution of the paper is the following logic\n        # section 4.1 - eq. 1\n        # first take care of the loss for all actions except for the very last one\n        q_pred_rest_actions, q_pred_last_action      = q_pred[..., :-1], q_pred[..., -1]\n        q_target_first_action, q_target_rest_actions = q_target[..., 0], q_target[..., 1:]\n        losses_all_actions_but_last = F.mse_loss(q_pred_rest_actions, q_target_rest_actions, reduction = 'none')\n        # next take care of the very last action, which incorporates the rewards\n        q_target_last_action, _ = pack([q_target_first_action[..., 1:], q_next], 'b *')\n        q_target_last_action = rewards + \u03b3 * q_target_last_action"
        },
        {
            "comment": "The code defines a Q-Learner class with methods for calculating Q-learning losses and performing Q-learning updates. The `learn` method takes in actions, monte_carlo_return, and min_reward as optional parameters. The code handles three types of Q-learning: 1) autoregressive q-learning for multiple actions (handles single or n-step automatically), 2) classic q-learning with a single action at each timestep, and 3) q-learning with a single action but considering n-steps. The method calculates the main Q-learning loss using these different types of Q-learning and returns the losses and intermediate results (q_pred_all_actions, q_pred, q_next, q_target).",
            "location": "\"/media/root/Prima/works/q-transformer/docs/src/q_transformer/q_learner.py\":496-525",
            "content": "        losses_last_action = F.mse_loss(q_pred_last_action, q_target_last_action, reduction = 'none')\n        # flatten and average\n        losses, _ = pack([losses_all_actions_but_last, losses_last_action], '*')\n        return losses.mean(), QIntermediates(q_pred_all_actions, q_pred, q_next, q_target)\n    def learn(\n        self,\n        *args,\n        min_reward: Optional[float] = None,\n        monte_carlo_return: Optional[float] = None\n    ):\n        _, _, actions, *_ = args\n        # q-learn kwargs\n        q_learn_kwargs = dict(\n            monte_carlo_return = monte_carlo_return\n        )\n        # main q-learning loss, respectively\n        # 1. proposed autoregressive q-learning for multiple actions - (handles single or n-step automatically)\n        # 2. single action - single timestep (classic q-learning)\n        # 3. single action - n-steps\n        if self.is_multiple_actions:\n            td_loss, q_intermediates = self.autoregressive_q_learn_handle_single_timestep(*args, **q_learn_kwargs)\n            num_timesteps = actions.shape[1]"
        },
        {
            "comment": "This code snippet checks whether n-step Q-learning or regular Q-learning is being used. It then calculates the TD loss and q_intermediates based on this. If conservative regularization is not enabled, it simply returns the loss and zero. Otherwise, it performs calculations to calculate conservative regularization according to section 4.2 in the paper (eq 2). This includes rearranging q_preds, creating action masks, and calculating q_actions_not_taken.",
            "location": "\"/media/root/Prima/works/q-transformer/docs/src/q_transformer/q_learner.py\":527-554",
            "content": "        elif self.n_step_q_learning:\n            td_loss, q_intermediates = self.n_step_q_learn(*args, **q_learn_kwargs)\n            num_timesteps = actions.shape[1]\n        else:\n            td_loss, q_intermediates = self.q_learn(*args, **q_learn_kwargs)\n            num_timesteps = 1\n        if not self.has_conservative_reg_loss:\n            return loss, Losses(td_loss, self.zero)\n        # calculate conservative regularization\n        # section 4.2 in paper, eq 2\n        batch = actions.shape[0]\n        q_preds = q_intermediates.q_pred_all_actions\n        q_preds = rearrange(q_preds, '... a -> (...) a')\n        num_action_bins = q_preds.shape[-1]\n        num_non_dataset_actions = num_action_bins - 1\n        actions = rearrange(actions, '... -> (...) 1')\n        dataset_action_mask = torch.zeros_like(q_preds).scatter_(-1, actions, torch.ones_like(q_preds))\n        q_actions_not_taken = q_preds[~dataset_action_mask.bool()]\n        q_actions_not_taken = rearrange(q_actions_not_taken, '(b t a) -> b t a', b = batch, a = num_non_dataset_actions)"
        },
        {
            "comment": "This code snippet is from the Q-Transformer model's QLearner class. It calculates the total loss by combining the TD loss and conservative regularization loss, which penalizes actions not present in the dataset. The forward method sets default values for monte_carlo_return and min_reward, trains the model, and iterates through a replay buffer for the number of training steps specified. Gradients are accumulated over the gradient accumulation steps before being updated.",
            "location": "\"/media/root/Prima/works/q-transformer/docs/src/q_transformer/q_learner.py\":556-592",
            "content": "        conservative_reg_loss = ((q_actions_not_taken - (min_reward * num_timesteps)) ** 2).sum() / num_non_dataset_actions\n        # total loss\n        loss =  0.5 * td_loss + \\\n                0.5 * conservative_reg_loss * self.conservative_reg_loss_weight\n        loss_breakdown = Losses(td_loss, conservative_reg_loss)\n        return loss, loss_breakdown\n    def forward(\n        self,\n        *,\n        monte_carlo_return: Optional[float] = None,\n        min_reward: Optional[float] = None\n    ):\n        monte_carlo_return = default(monte_carlo_return, self.monte_carlo_return)\n        min_reward = default(min_reward, self.min_reward)\n        step = self.step.item()\n        replay_buffer_iter = cycle(self.dataloader)\n        self.model.train()\n        self.ema_model.train()\n        while step < self.num_train_steps:\n            # zero grads\n            self.optimizer.zero_grad()\n            # main q-learning algorithm\n            for grad_accum_step in range(self.grad_accum_every):\n                is_last = grad_accum_step == (self.grad_accum_every - 1)"
        },
        {
            "comment": "The code is training a model using an accelerator for synchronization and automatic differentiation. It applies loss calculation, gradient clipping (Transformer best practice), optimizer step, updates the target ema (exponential moving average) model, increments the step counter, and checks if it's time to perform a checkpointing operation. The code also handles asynchronous computation by using partial application of accelerator's no_sync method on the model for non-last iterations.",
            "location": "\"/media/root/Prima/works/q-transformer/docs/src/q_transformer/q_learner.py\":593-630",
            "content": "                context = partial(self.accelerator.no_sync, self.model) if not is_last else nullcontext\n                with self.accelerator.autocast(), context():\n                    loss, (td_loss, conservative_reg_loss) = self.learn(\n                        *next(replay_buffer_iter),\n                        min_reward = min_reward,\n                        monte_carlo_return = monte_carlo_return\n                    )\n                    self.accelerator.backward(loss / self.grad_accum_every)\n            self.print(f'td loss: {td_loss.item():.3f}')\n            # clip gradients (transformer best practices)\n            self.accelerator.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)\n            # take optimizer step\n            self.optimizer.step()\n            # update target ema\n            self.wait()\n            self.ema_model.update()\n            # increment step\n            step += 1\n            self.step.add_(1)\n            # whether to checkpoint or not\n            self.wait()\n            if self.is_main and is_divisible(step, self.checkpoint_every):"
        },
        {
            "comment": "The code saves a checkpoint every self.checkpoint_every steps, then waits before printing \"training complete\".",
            "location": "\"/media/root/Prima/works/q-transformer/docs/src/q_transformer/q_learner.py\":631-636",
            "content": "                checkpoint_num = step // self.checkpoint_every\n                self.save(checkpoint_num)\n            self.wait()\n        self.print('training complete')"
        }
    ]
}