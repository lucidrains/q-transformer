{
    "summary": "The code installs the scalable offline reinforcement learning model, Q-Transformer, with encoder-decoder architecture and various action decoding techniques. It discusses RL expert consultations and mentions publications on delusional bias, randomized exploration, and \"FlashAttention\" manuscript.",
    "details": [
        {
            "comment": "This code is for installing and using the \"q-transformer\" package, which implements Q-Transformer - a scalable offline reinforcement learning model from Google DeepMind. The model uses an attention mechanism in its Q-function to predict multiple actions at once, improving performance compared to traditional single action Q-learning. The code installs the package via pip and provides an example of how to use it with PyTorch, specifying the details of the attention model and Q-function.",
            "location": "\"/media/root/Prima/works/q-transformer/docs/src/README.md\":0-43",
            "content": "<img src=\"./q-transformer.png\" width=\"450px\"></img>\n## Q-transformer\nImplementation of <a href=\"https://qtransformer.github.io/\">Q-Transformer</a>, Scalable Offline Reinforcement Learning via Autoregressive Q-Functions, out of Google Deepmind\nI will be keeping around the logic for Q-learning on single action just for final comparison with the proposed autoregressive Q-learning on multiple actions. Also to serve as education for myself and the public.\n## Install\n```bash\n$ pip install q-transformer\n```\n## Usage\n```python\nimport torch\nfrom q_transformer import (\n    QRoboticTransformer,\n    QLearner,\n    Agent,\n    ReplayMemoryDataset\n)\n# the attention model\nmodel = QRoboticTransformer(\n    vit = dict(\n        num_classes = 1000,\n        dim_conv_stem = 64,\n        dim = 64,\n        dim_head = 64,\n        depth = (2, 2, 5, 2),\n        window_size = 7,\n        mbconv_expansion_rate = 4,\n        mbconv_shrinkage_rate = 0.25,\n        dropout = 0.1\n    ),\n    num_actions = 8,\n    action_bins = 256,\n    depth = 1,\n    heads = 8,"
        },
        {
            "comment": "The code initializes an environment (MockEnvironment) and two learning components: an Agent and a QLearner. The Agent interacts with the environment to generate a replay memory dataset for the model, and then the QLearner learns from this replay memory dataset. After much learning, the robot should be better at selecting optimal actions.",
            "location": "\"/media/root/Prima/works/q-transformer/docs/src/README.md\":44-85",
            "content": "    dim_head = 64,\n    cond_drop_prob = 0.2,\n    dueling = True\n)\n# you need to supply your own environment, by overriding BaseEnvironment\nfrom q_transformer.mocks import MockEnvironment\nenv = MockEnvironment(\n    state_shape = (3, 6, 224, 224),\n    text_embed_shape = (768,)\n)\n# env.init()     should return instructions and initial state: Tuple[str, Tensor[*state_shape]]\n# env(actions)   should return rewards, next state, and done flag: Tuple[Tensor[()], Tensor[*state_shape], Tensor[()]]\n# agent is a class that allows the q-model to interact with the environment to generate a replay memory dataset for learning\nagent = Agent(\n    model,\n    environment = env,\n    num_episodes = 10000,\n    max_num_steps_per_episode = 1000,\n)\nagent()\n# Q learning on the replay memory dataset on the model\nq_learner = QLearner(\n    model,\n    dataset = ReplayMemoryDataset(),\n    num_train_steps = 10000,\n    learning_rate = 3e-4,\n    batch_size = 32\n)\nq_learner()\n# after much learning\n# your robot should be better at selecting optimal actions"
        },
        {
            "comment": "The code initializes a random video, a list of instructions, and then calls the `get_optimal_actions` function from the model to get the actions for the given instructions. The model returns the optimal actions based on the provided video and instructions.",
            "location": "\"/media/root/Prima/works/q-transformer/docs/src/README.md\":87-110",
            "content": "video = torch.randn(2, 3, 6, 224, 224)\ninstructions = [\n    'bring me that apple sitting on the table',\n    'please pass the butter'\n]\nactions = model.get_optimal_actions(video, instructions)\n```\n## Appreciation\n- <a href=\"https://stability.ai/\">StabilityAI</a>, <a href=\"https://a16z.com/supporting-the-open-source-ai-community/\">A16Z Open Source AI Grant Program</a>, and <a href=\"https://huggingface.co/\">\ud83e\udd17 Huggingface</a> for the generous sponsorships, as well as my other sponsors, for affording me the independence to open source current artificial intelligence research\n## Todo\n- [x] first work way towards single action support\n- [x] offer batchnorm-less variant of maxvit, as done in SOTA weather model metnet3\n- [x] add optional deep dueling architecture\n- [x] add n-step Q learning\n- [x] build the conservative regularization\n- [x] build out main proposal in paper (autoregressive discrete actions until last action, reward given only on last)\n- [x] improvise decoder head variant, instead of concatenatin"
        },
        {
            "comment": "This code outlines tasks for building a model that uses classic encoder-decoder architecture, allowing for cross attention to fine frames and learned tokens. It mentions adding axial rotary embeddings and sigmoid gating for attending to nothing. The code also includes building a simple dataset creator class, handling multiple instructions correctly, and creating an end-to-end example. It caches kv for action decoding, considers no instructions using null conditioner, and explores finely randomizing actions or using Gumbel-based sampling with annealing noise.",
            "location": "\"/media/root/Prima/works/q-transformer/docs/src/README.md\":110-124",
            "content": "g previous actions at the frames + learned tokens stage. in other words, use classic encoder - decoder\n    - [x] allow for cross attention to fine frame / learned tokens\n- [x] redo maxvit with axial rotary embeddings + sigmoid gating for attending to nothing. enable flash attention for maxvit with this change\n- [x] build out a simple dataset creator class, taking in the environment and model and returning a folder that can be accepted by a `ReplayDataset`\n    - [x] finish basic environment loop\n    - [x] store memories to memmapped files in designated folder\n    - [x] `ReplayDataset` that takes in folder\n        - [x] 1 time step option\n        - [x] n-time steps\n- [x] handle multiple instructions correctly\n- [x] show a simple end-to-end example, in the same style as all other repos\n- [x] handle no instructions, leverage null conditioner in CFG library\n- [x] cache kv for action decoding\n- [x] for exploration, allow for finely randomizing a subset of actions, and not all actions at once\n    - [ ] also allow for gumbel based sampling of actions, with annealing of gumbel noise"
        },
        {
            "comment": "This code snippet appears to be a list of tasks or features for a Q-Transformer model, as mentioned in the \"q-transformer/README.md\" file. The tasks include consulting RL experts on delusional bias, exploring randomized action orders, suggesting an improvised variant with varying attention to past actions, creating a beam search function for optimal actions, adding cross attention like Transformer-XL, and potentially applying the main idea from a specific paper to language models.",
            "location": "\"/media/root/Prima/works/q-transformer/docs/src/README.md\":126-137",
            "content": "- [ ] consult some RL experts and figure out if there are any new headways into resolving <a href=\"https://www.cs.toronto.edu/~cebly/Papers/CONQUR_ICML_2020_camera_ready.pdf\">delusional bias</a>\n- [ ] figure out if one can train with randomized orders of actions - order could be sent as a conditioning that is concatted or summed before attention layers\n    - [ ] offer an improvised variant where the first action token suggests the action ordering. all actions aren't made equal, and some may need to attend to past actions more than others\n- [ ] simple beam search function for optimal actions\n- [ ] improvise cross attention to past actions and states of timestep, transformer-xl fashion (w/ structured memory dropout)\n- [ ] see if the main idea in this paper is applicable to language models <a href=\"https://github.com/lucidrains/llama-qrlhf\">here</a>\n## Citations\n```bibtex\n@inproceedings{qtransformer,\n    title   = {Q-Transformer: Scalable Offline Reinforcement Learning via Autoregressive Q-Functions},"
        },
        {
            "comment": "This code snippet contains two entries for academic publications, formatted in the BibTeX style. The first entry appears to be a conference paper from the 7th Annual Conference on Robot Learning with multiple authors. The second entry is an unpublished manuscript titled \"FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness\" by four authors, published in the Advances in Neural Information Processing Systems conference in 2022.",
            "location": "\"/media/root/Prima/works/q-transformer/docs/src/README.md\":138-151",
            "content": "    authors = {Yevgen Chebotar and Quan Vuong and Alex Irpan and Karol Hausman and Fei Xia and Yao Lu and Aviral Kumar and Tianhe Yu and Alexander Herzog and Karl Pertsch and Keerthana Gopalakrishnan and Julian Ibarz and Ofir Nachum and Sumedh Sontakke and Grecia Salazar and Huong T Tran and Jodilyn Peralta and Clayton Tan and Deeksha Manjunath and Jaspiar Singht and Brianna Zitkovich and Tomas Jackson and Kanishka Rao and Chelsea Finn and Sergey Levine},\n    booktitle = {7th Annual Conference on Robot Learning},\n    year   = {2023}\n}\n```\n```bibtex\n@inproceedings{dao2022flashattention,\n    title   = {Flash{A}ttention: Fast and Memory-Efficient Exact Attention with {IO}-Awareness},\n    author  = {Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and R{\\'e}, Christopher},\n    booktitle = {Advances in Neural Information Processing Systems},\n    year    = {2022}\n}\n```"
        }
    ]
}