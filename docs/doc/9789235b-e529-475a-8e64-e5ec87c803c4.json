{
    "summary": "This code defines two functions: `separate_weight_decayable_params` and `get_adam_optimizer`. The first function separates parameters into those with weight decay (`wd_params`) and those without (`no_wd_params`). The second function gets an Adam optimizer based on the given parameters, learning rate (lr), weight decay (wd), betas, epsilon, whether to filter by requires_grad, and whether to group wd_params. If weight decay is greater than 0, it separates parameters into groups with and without weight decay. Then, it returns an Adam optimizer if there's no weight decay, or an AdamW optimizer if there is weight decay.",
    "details": [
        {
            "comment": "This code defines two functions: `separate_weight_decayable_params` and `get_adam_optimizer`. The first function separates parameters into those with weight decay (`wd_params`) and those without (`no_wd_params`). The second function gets an Adam optimizer based on the given parameters, learning rate (lr), weight decay (wd), betas, epsilon, whether to filter by requires_grad, and whether to group wd_params. If weight decay is greater than 0, it separates parameters into groups with and without weight decay. Then, it returns an Adam optimizer if there's no weight decay, or an AdamW optimizer if there is weight decay.",
            "location": "\"/media/root/Prima/works/q-transformer/docs/src/q_transformer/optimizer.py\":0-34",
            "content": "from torch.optim import AdamW, Adam\ndef separate_weight_decayable_params(params):\n    wd_params, no_wd_params = [], []\n    for param in params:\n        param_list = no_wd_params if param.ndim < 2 else wd_params\n        param_list.append(param)\n    return wd_params, no_wd_params\ndef get_adam_optimizer(\n    params,\n    lr = 1e-4,\n    wd = 1e-2,\n    betas = (0.9, 0.99),\n    eps = 1e-8,\n    filter_by_requires_grad = False,\n    group_wd_params = True\n):\n    has_wd = wd > 0\n    if filter_by_requires_grad:\n        params = list(filter(lambda t: t.requires_grad, params))\n    if group_wd_params and has_wd:\n        wd_params, no_wd_params = separate_weight_decayable_params(params)\n        params = [\n            {'params': wd_params},\n            {'params': no_wd_params, 'weight_decay': 0},\n        ]\n    if not has_wd:\n        return Adam(params, lr = lr, betas = betas, eps = eps)\n    return AdamW(params, lr = lr, weight_decay = wd, betas = betas, eps = eps)"
        }
    ]
}