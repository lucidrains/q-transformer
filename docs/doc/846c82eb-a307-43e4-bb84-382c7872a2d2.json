{
    "summary": "This code defines a class for attention-based neural network operations using PyTorch, incorporating dropout regularization and optional flash attention for efficiency. It computes attention scores, applies masks and dropout, uses scaled dot product mechanism, and aggregates values with Einstein summation.",
    "details": [
        {
            "comment": "This code defines a class called Attend, which is a neural network module for attention-based operations. It uses the PyTorch framework and includes several helper functions for dropout, masking, and processing multiple masks. The class has configurable parameters for dropout rate, causal attention, and flash operation configuration (enable_flash, enable_math, enable_mem_efficient).",
            "location": "\"/media/root/Prima/works/q-transformer/docs/src/q_transformer/attend.py\":0-57",
            "content": "from functools import wraps\nfrom packaging import version\nimport torch\nfrom torch import nn, einsum\nimport torch.nn.functional as F\nfrom einops import rearrange, reduce\n# helpers\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\nprint_once = once(print)\n# helpers\ndef exists(val):\n    return val is not None\ndef default(val, d):\n    return val if exists(val) else d\ndef maybe_reduce_mask_and(*maybe_masks):\n    maybe_masks = [*filter(exists, maybe_masks)]\n    if len(maybe_masks) == 0:\n        return None\n    mask, *rest_masks = maybe_masks\n    for rest_mask in rest_masks:\n        mask = mask & rest_mask\n    return mask\n# main class\nclass Attend(nn.Module):\n    def __init__(\n        self,\n        dropout = 0.,\n        flash = False,\n        causal = False,\n        flash_config: dict = dict(\n            enable_flash = True,\n            enable_math = True,\n            enable_mem_efficient = True\n        )"
        },
        {
            "comment": "This code is defining a class for a self-attention mechanism, which uses dropout regularization and has an optional flash attention implementation for memory efficiency. The flash_attn method performs the self-attention operation, taking in queries (q), keys (k), and values (v) along with optional masking. If a mask exists, it is expanded to the required shape before applying the self-attention operation using pytorch 2.0's flash attention implementation.",
            "location": "\"/media/root/Prima/works/q-transformer/docs/src/q_transformer/attend.py\":58-85",
            "content": "    ):\n        super().__init__()\n        self.dropout = dropout\n        self.attn_dropout = nn.Dropout(dropout)\n        self.causal = causal\n        self.flash = flash\n        assert not (flash and version.parse(torch.__version__) < version.parse('2.0.0')), 'in order to use flash attention, you must be using pytorch 2.0 or above'\n        if flash:\n            print_once('using memory efficient attention')\n        self.flash_config = flash_config\n    def flash_attn(self, q, k, v, mask = None, attn_mask = None):\n        _, heads, q_len, dim_head, k_len, is_cuda, device = *q.shape, k.shape[-2], q.is_cuda, q.device\n        # Check if mask exists and expand to compatible shape\n        # The mask is B L, so it would have to be expanded to B H N L\n        if exists(mask):\n            mask = mask.expand(-1, heads, q_len, -1)\n        mask = maybe_reduce_mask_and(mask, attn_mask)\n        # pytorch 2.0 flash attn: q, k, v, mask, dropout, softmax_scale\n        with torch.backends.cuda.sdp_kernel(**self.flash_config):"
        },
        {
            "comment": "The function calculates the attention scores and returns them, applying causality and dropout if necessary. It uses scaled dot product attention mechanism in Einstein notation, performs scaling of the queries, applies masking if provided, and has an optional flash attn implementation. The code also handles causal masks by using a triangle upper matrix and applies dropout during training.",
            "location": "\"/media/root/Prima/works/q-transformer/docs/src/q_transformer/attend.py\":86-122",
            "content": "            out = F.scaled_dot_product_attention(\n                q, k, v,\n                attn_mask = mask,\n                is_causal = self.causal,\n                dropout_p = self.dropout if self.training else 0.\n            )\n        return out\n    def forward(self, q, k, v, mask = None, attn_mask = None):\n        \"\"\"\n        einstein notation\n        b - batch\n        h - heads\n        n, i, j - sequence length (base sequence length, source, target)\n        d - feature dimension\n        \"\"\"\n        q_len, k_len, device = q.shape[-2], k.shape[-2], q.device\n        scale = q.shape[-1] ** -0.5\n        if exists(mask) and mask.ndim != 4:\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n        if self.flash:\n            return self.flash_attn(q, k, v, mask = mask, attn_mask = attn_mask)\n        # similarity\n        sim = einsum(f\"b h i d, b h j d -> b h i j\", q, k) * scale\n        # causal mask\n        if self.causal:\n            i, j = sim.shape[-2:]\n            causal_mask = torch.ones((i, j), dtype = torch.bool, device = sim.device).triu(j - i + 1)"
        },
        {
            "comment": "This code computes the attention scores and applies masks before computing attention. It fills values that should be ignored with the maximum possible negative value to prevent influence on the results. Then, it softmax normalizes the attention scores and applies dropout for regularization. Finally, it aggregates the values using Einstein summation.",
            "location": "\"/media/root/Prima/works/q-transformer/docs/src/q_transformer/attend.py\":123-144",
            "content": "            sim = sim.masked_fill(causal_mask, -torch.finfo(sim.dtype).max)\n        # key padding mask\n        if exists(mask):\n            sim = sim.masked_fill(~mask, -torch.finfo(sim.dtype).max)\n        # attention mask\n        if exists(attn_mask):\n            sim = sim.masked_fill(~attn_mask, -torch.finfo(sim.dtype).max)\n        # attention\n        attn = sim.softmax(dim=-1)\n        attn = self.attn_dropout(attn)\n        # aggregate values\n        out = einsum(f\"b h i j, b h j d -> b h i d\", attn, v)\n        return out"
        }
    ]
}