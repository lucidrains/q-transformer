{
    "summary": "This code initializes and trains a Q-Transformer agent in a reinforcement learning task, storing experiences in memory buffers. It handles environment details, epsilon parameters, rewards, actions, and memories for each episode until completion.",
    "details": [
        {
            "comment": "The code defines a `ReplayMemoryDataset` class, which is a subclass of `torch.utils.data.Dataset`. It loads data from memory maps in specified filenames and uses the given folder for storing these memory maps. The constructor also takes an optional argument `num_timesteps`, which should be greater than or equal to 1.",
            "location": "\"/media/root/Prima/works/q-transformer/docs/src/q_transformer/agent.py\":0-47",
            "content": "from pathlib import Path\nfrom numpy.lib.format import open_memmap\nimport torch\nfrom torch import nn, einsum, Tensor\nfrom torch.nn import Module, ModuleList\nfrom torch.utils.data import Dataset\nfrom einops import rearrange\nfrom q_transformer.q_robotic_transformer import QRoboticTransformer\nfrom torchtyping import TensorType\nfrom beartype import beartype\nfrom beartype.typing import Iterator, Tuple, Union\nfrom tqdm import tqdm\n# constants\nTEXT_EMBEDS_FILENAME = 'text_embeds.memmap.npy'\nSTATES_FILENAME = 'states.memmap.npy'\nACTIONS_FILENAME = 'actions.memmap.npy'\nREWARDS_FILENAME = 'rewards.memmap.npy'\nDONES_FILENAME = 'dones.memmap.npy'\nDEFAULT_REPLAY_MEMORIES_FOLDER = './replay_memories_data'\n# helpers\ndef exists(v):\n    return v is not None\ndef cast_tuple(t):\n    return (t,) if not isinstance(t, tuple) else t\n# replay memory dataset\nclass ReplayMemoryDataset(Dataset):\n    @beartype\n    def __init__(\n        self,\n        folder: str = DEFAULT_REPLAY_MEMORIES_FOLDER,\n        num_timesteps: int = 1\n    ):\n        assert num_timesteps >= 1"
        },
        {
            "comment": "This code initializes the agent's attributes by checking if the given folder exists and is a directory. It then opens memory-mapped files for text embeddings, states, actions, rewards, and done signals in read mode. The episode length is calculated based on dones, and the number of timesteps is set.",
            "location": "\"/media/root/Prima/works/q-transformer/docs/src/q_transformer/agent.py\":48-71",
            "content": "        self.is_single_timestep = num_timesteps == 1\n        self.num_timesteps = num_timesteps\n        folder = Path(folder)\n        assert folder.exists() and folder.is_dir()\n        text_embeds_path = folder / TEXT_EMBEDS_FILENAME\n        states_path = folder / STATES_FILENAME\n        actions_path = folder / ACTIONS_FILENAME\n        rewards_path = folder / REWARDS_FILENAME\n        dones_path = folder / DONES_FILENAME\n        self.text_embeds = open_memmap(str(text_embeds_path), dtype = 'float32', mode = 'r')\n        self.states = open_memmap(str(states_path), dtype = 'float32', mode = 'r')\n        self.actions = open_memmap(str(actions_path), dtype = 'int', mode = 'r')\n        self.rewards = open_memmap(str(rewards_path), dtype = 'float32', mode = 'r')\n        self.dones = open_memmap(str(dones_path), dtype = 'bool', mode = 'r')\n        self.num_timesteps = num_timesteps\n        # calculate episode length based on dones\n        # filter out any episodes that are insufficient in length\n        self.episode_length = (self.dones.cumsum(axis = -1) == 0).sum(axis = -1) + 1"
        },
        {
            "comment": "This code segment is part of an agent's class, which involves training episodes based on the given number of timesteps. It selects trainable episodes based on the episode length and creates trainable mask and indices for further processing.",
            "location": "\"/media/root/Prima/works/q-transformer/docs/src/q_transformer/agent.py\":73-98",
            "content": "        trainable_episode_indices = self.episode_length >= num_timesteps\n        self.text_embeds = self.text_embeds[trainable_episode_indices]\n        self.states = self.states[trainable_episode_indices]\n        self.actions = self.actions[trainable_episode_indices]\n        self.rewards = self.rewards[trainable_episode_indices]\n        self.dones = self.dones[trainable_episode_indices]\n        self.episode_length = self.episode_length[trainable_episode_indices]\n        assert self.dones.size > 0, 'no trainable episodes'\n        self.num_episodes, self.max_episode_len = self.dones.shape\n        timestep_arange = torch.arange(self.max_episode_len)\n        timestep_indices = torch.stack(torch.meshgrid(\n            torch.arange(self.num_episodes),\n            timestep_arange\n        ), dim = -1)\n        trainable_mask = timestep_arange < rearrange(torch.from_numpy(self.episode_length) - num_timesteps, 'e -> e 1')\n        self.indices = timestep_indices[trainable_mask]\n    def __len__(self):\n        return self.indices.shape[0]"
        },
        {
            "comment": "The code defines a BaseEnvironment class that extends a module, takes state_shape and text_embed_shape as input parameters, and registers a buffer called 'dummy'. The __getitem__ method returns text_embeds, states, actions, next_state, rewards, and dones based on the provided index.",
            "location": "\"/media/root/Prima/works/q-transformer/docs/src/q_transformer/agent.py\":100-128",
            "content": "    def __getitem__(self, idx):\n        episode_index, timestep_index = self.indices[idx]\n        timestep_slice = slice(timestep_index, (timestep_index + self.num_timesteps))\n        text_embeds = self.text_embeds[episode_index, timestep_slice]\n        states = self.states[episode_index, timestep_slice]\n        actions = self.actions[episode_index, timestep_slice]\n        rewards = self.rewards[episode_index, timestep_slice]\n        dones = self.dones[episode_index, timestep_slice]\n        next_state = self.states[episode_index, min(timestep_index, self.max_episode_len - 1)]\n        return text_embeds, states, actions, next_state, rewards, dones\n# base environment class to extend\nclass BaseEnvironment(Module):\n    @beartype\n    def __init__(\n        self,\n        *,\n        state_shape: Tuple[int, ...],\n        text_embed_shape: Union[int, Tuple[int, ...]]\n    ):\n        super().__init__()\n        self.state_shape = state_shape\n        self.text_embed_shape = cast_tuple(text_embed_shape)\n        self.register_buffer('dummy', torch.zeros(0), persistent = False)"
        },
        {
            "comment": "The code defines an Agent class that takes a QRoboticTransformer as input and initializes its attributes. It also sets the condition_on_text attribute from the q_transformer's condition_on_text value. The class has not implemented the init, forward, or device methods yet.",
            "location": "\"/media/root/Prima/works/q-transformer/docs/src/q_transformer/agent.py\":130-167",
            "content": "    @property\n    def device(self):\n        return self.dummy.device\n    def init(self) -> Tuple[str, Tensor]: # (instruction, initial state)\n        raise NotImplementedError\n    def forward(\n        self,\n        actions: Tensor\n    ) -> Tuple[\n        TensorType[(), float],     # reward\n        Tensor,                    # next state\n        TensorType[(), bool]       # done\n    ]:\n        raise NotImplementedError\n# agent class\nclass Agent(Module):\n    @beartype\n    def __init__(\n        self,\n        q_transformer: QRoboticTransformer,\n        *,\n        environment: BaseEnvironment,\n        memories_dataset_folder: str = DEFAULT_REPLAY_MEMORIES_FOLDER,\n        num_episodes: int = 1000,\n        max_num_steps_per_episode: int = 10000,\n        epsilon_start: float = 0.25,\n        epsilon_end: float = 0.001,\n        num_steps_to_target_epsilon: int = 1000\n    ):\n        super().__init__()\n        self.q_transformer = q_transformer\n        condition_on_text = q_transformer.condition_on_text\n        self.condition_on_text = condition_on_text"
        },
        {
            "comment": "This code initializes an agent for a Q-transformer, setting environment details and epsilon parameters. It also creates memory file paths for states, actions, rewards, and done flags, defining their shapes based on number of episodes and max steps per episode.",
            "location": "\"/media/root/Prima/works/q-transformer/docs/src/q_transformer/agent.py\":169-196",
            "content": "        self.environment = environment\n        assert hasattr(environment, 'state_shape') and hasattr(environment, 'text_embed_shape')\n        assert 0. <= epsilon_start <= 1.\n        assert 0. <= epsilon_end <= 1.\n        assert epsilon_start >= epsilon_end\n        self.epsilon_start = epsilon_start\n        self.epsilon_end = epsilon_end\n        self.num_steps_to_target_epsilon = num_steps_to_target_epsilon\n        self.epsilon_slope = (epsilon_end - epsilon_start) / num_steps_to_target_epsilon\n        self.num_episodes = num_episodes\n        self.max_num_steps_per_episode = max_num_steps_per_episode\n        mem_path = Path(memories_dataset_folder)\n        self.memories_dataset_folder = mem_path\n        mem_path.mkdir(exist_ok = True, parents = True)\n        assert mem_path.is_dir()\n        states_path = mem_path / STATES_FILENAME\n        actions_path = mem_path / ACTIONS_FILENAME\n        rewards_path = mem_path / REWARDS_FILENAME\n        dones_path = mem_path / DONES_FILENAME\n        prec_shape = (num_episodes, max_num_steps_per_episode)"
        },
        {
            "comment": "The code defines a class with methods for initializing memory maps for states, actions, rewards, and done status. It also includes a method to get the epsilon value at a given step. The epsilon value is calculated using a linear function that increases from start to end values. The code assumes certain environment properties like state_shape, num_actions, and text_embed_shape are defined beforehand.",
            "location": "\"/media/root/Prima/works/q-transformer/docs/src/q_transformer/agent.py\":197-213",
            "content": "        num_actions = q_transformer.num_actions\n        state_shape = environment.state_shape\n        if condition_on_text:\n            text_embeds_path = mem_path / TEXT_EMBEDS_FILENAME\n            text_embed_shape = environment.text_embed_shape\n            self.text_embed_shape = text_embed_shape\n            self.text_embeds = open_memmap(str(text_embeds_path), dtype = 'float32', mode = 'w+', shape = (*prec_shape, *text_embed_shape))\n        self.states      = open_memmap(str(states_path), dtype = 'float32', mode = 'w+', shape = (*prec_shape, *state_shape))\n        self.actions     = open_memmap(str(actions_path), dtype = 'int', mode = 'w+', shape = (*prec_shape, num_actions))\n        self.rewards     = open_memmap(str(rewards_path), dtype = 'float32', mode = 'w+', shape = prec_shape)\n        self.dones       = open_memmap(str(dones_path), dtype = 'bool', mode = 'w+', shape = prec_shape)\n    def get_epsilon(self, step):\n        return max(self.epsilon_end, self.epsilon_slope * float(step) + self.epsilon_start)"
        },
        {
            "comment": "This code defines the forward function for an agent in a Q-Transformer model. It evaluates the Q-Transformer, iterates through episodes, and performs actions based on whether to condition on text or not. The agent interacts with an environment, receives rewards and next states, and stores memories using memmap for later reflection and learning.",
            "location": "\"/media/root/Prima/works/q-transformer/docs/src/q_transformer/agent.py\":215-247",
            "content": "    @beartype\n    @torch.no_grad()\n    def forward(self):\n        self.q_transformer.eval()\n        for episode in range(self.num_episodes):\n            print(f'episode {episode}')\n            instruction, curr_state = self.environment.init()\n            for step in tqdm(range(self.max_num_steps_per_episode)):\n                last_step = step == (self.max_num_steps_per_episode - 1)\n                epsilon = self.get_epsilon(step)\n                text_embed = None\n                if self.condition_on_text:\n                    text_embed = self.q_transformer.embed_texts([instruction])\n                actions = self.q_transformer.get_actions(\n                    rearrange(curr_state, '... -> 1 ...'),\n                    text_embeds = text_embed,\n                    prob_random_action = epsilon\n                )\n                reward, next_state, done = self.environment(actions)\n                done = done | last_step\n                # store memories using memmap, for later reflection and learning\n                if self.condition_on_text:"
        },
        {
            "comment": "This code snippet stores experiences of an agent in a reinforcement learning task. It checks the shape of text embeddings, saves the current state, action taken, reward received, and done status for each step in episodes. If done, it breaks the loop and moves onto the next episode. After all steps are processed, it flushes memory buffers and prints a completion message with the storage folder location.",
            "location": "\"/media/root/Prima/works/q-transformer/docs/src/q_transformer/agent.py\":248-273",
            "content": "                    assert text_embed.shape[1:] == self.text_embed_shape\n                    self.text_embeds[episode, step] = text_embed\n                self.states[episode, step]      = curr_state\n                self.actions[episode, step]     = actions\n                self.rewards[episode, step]     = reward\n                self.dones[episode, step]       = done\n                # if done, move onto next episode\n                if done:\n                    break\n                # set next state\n                curr_state = next_state\n            if self.condition_on_text:\n                self.text_embeds.flush()\n            self.states.flush()\n            self.actions.flush()\n            self.rewards.flush()\n            self.dones.flush()\n        print(f'completed, memories stored to {self.memories_dataset_folder.resolve()}')"
        }
    ]
}