{
    "summary": "This class creates mock environments and datasets for Q-transformer models in reinforcement learning. The custom dataloader supports multiple steps per experience, stochastic actions, and defined time_shape, num_action_bins, and video_shape.",
    "details": [
        {
            "comment": "This code defines a mock environment and replay dataset for a Q-transformer model. The MockEnvironment class has an init method that returns an initial state and reward, and a forward method that returns rewards, next states, and done signals. The MockReplayDataset class initializes with parameters for length, number of actions, action bins, and video shape.",
            "location": "\"/media/root/Prima/works/q-transformer/docs/src/q_transformer/mocks.py\":0-37",
            "content": "from random import randrange\nimport torch\nfrom torch.utils.data import Dataset\nfrom beartype.typing import Tuple, Optional\nfrom torchtyping import TensorType\nfrom q_transformer.agent import BaseEnvironment\nclass MockEnvironment(BaseEnvironment):\n    def init(self) -> Tuple[\n        Optional[str],\n        TensorType[float]\n    ]:\n        return 'please clean the kitchen', torch.randn(self.state_shape, device = self.device)\n    def forward(self, actions) -> Tuple[\n        TensorType[(), float],\n        TensorType[float],\n        TensorType[(), bool]\n    ]:\n        rewards = torch.randn((), device = self.device)\n        next_states = torch.randn(self.state_shape, device = self.device)\n        done = torch.zeros((), device = self.device, dtype = torch.bool)\n        return rewards, next_states, done\nclass MockReplayDataset(Dataset):\n    def __init__(\n        self,\n        length = 10000,\n        num_actions = 1,\n        num_action_bins = 256,\n        video_shape = (6, 224, 224)\n    ):\n        self.length = length\n        self.num_actions = num_actions"
        },
        {
            "comment": "The code defines a class, \"MockReplayNStepDataset\", that generates a dataset of experiences for training a reinforcement learning agent. It has parameters such as length (number of experiences), num_steps (number of timesteps per experience), num_actions (number of actions per timestep), num_action_bins (number of possible action bins), and video_shape (size of the video frame). The class initializes these parameters and defines a __getitem__ method that returns an experience for a given index. It also includes a nested class \"MockActionDistribution\" for handling actions, which can be either deterministic or stochastic.",
            "location": "\"/media/root/Prima/works/q-transformer/docs/src/q_transformer/mocks.py\":38-72",
            "content": "        self.num_action_bins = num_action_bins\n        self.video_shape = video_shape\n    def __len__(self):\n        return self.length\n    def __getitem__(self, _):\n        instruction = \"please clean the kitchen\"\n        state = torch.randn(3, *self.video_shape)\n        if self.num_actions == 1:\n            action = torch.tensor(randrange(self.num_action_bins + 1))\n        else:\n            action = torch.randint(0, self.num_action_bins + 1, (self.num_actions,))\n        next_state = torch.randn(3, *self.video_shape)\n        reward = torch.tensor(randrange(2))\n        done = torch.tensor(randrange(2), dtype = torch.bool)\n        return instruction, state, action, next_state, reward, done\nclass MockReplayNStepDataset(Dataset):\n    def __init__(\n        self,\n        length = 10000,\n        num_steps = 2,\n        num_actions = 1,\n        num_action_bins = 256,\n        video_shape = (6, 224, 224)\n    ):\n        self.num_steps = num_steps\n        self.time_shape = (num_steps,)\n        self.length = length\n        self.num_actions = num_actions"
        },
        {
            "comment": "This class is a custom dataloader for RL tasks, with time_shape, num_action_bins, and video_shape defined. It defines __len__ and __getitem__ methods to return instructions, states, actions, next_states, rewards, and done masks in each iteration. The state shape is (time, 3, *video_shape), and the action bins are randomly sampled based on num_action_bins.",
            "location": "\"/media/root/Prima/works/q-transformer/docs/src/q_transformer/mocks.py\":73-90",
            "content": "        self.num_action_bins = num_action_bins\n        self.video_shape = video_shape\n    def __len__(self):\n        return self.length\n    def __getitem__(self, _):\n        action_dims = (self.num_actions,) if self.num_actions > 1 else tuple()\n        instruction = \"please clean the kitchen\"\n        state = torch.randn(*self.time_shape, 3, *self.video_shape)\n        action = torch.randint(0, self.num_action_bins + 1, (*self.time_shape, *action_dims))\n        next_state = torch.randn(3, *self.video_shape)\n        reward = torch.randint(0, 2, self.time_shape)\n        done = torch.zeros(self.time_shape, dtype = torch.bool)\n        return instruction, state, action, next_state, reward, done"
        }
    ]
}